Course Overview
Course Overview
Hi, my name is Janani Ravi, and welcome to this course on Preparing Data for Feature Engineering and Machine Learning. A little about myself, I have a master's degree in electrical engineering from Stanford and have worked at companies such as Microsoft, Google, and Flipkart. At Google, I was one of the first engineers working on real time collaborative editing in Google Docs and I hold four patents for its underlying technologies. I currently work on my own startup, Loonycorn, a studio for high quality video content. However well designed and well implemented a machine learning model is, if the data fed in is poorly engineered, the model's predictions will be disappointing. In this course, you will gain the ability to appropriately pre-process your data, in effect engineer it so that you can get the best out of your ML models. First, you will learn how feature selection techniques can be used to find predictors that contain the most information. Feature selection can be broadly grouped into three categories known as filter, wrapper, and embedded techniques, and we will understand and implement all of these. Next, you will discover how feature extraction defers from feature selection in that data is substantially re-expressed, sometimes in forms that are hard to interpret. You will then understand techniques for feature extraction from image and text data. Finally, you will round out your knowledge by understanding how to leverage powerful Python libraries for working with images, text, dates, and geospatial data. When you're finished with this course, you will have the skills and knowledge to identify the correct feature engineering techniques and the appropriate solutions for your use case.

Understanding the Role of Features in Machine Learning
Version Check
Module Overview
Hi, and welcome to this course on Preparing Data for Feature Engineering and Machine Learning. In this module, we'll understand the rule of features in machine learning. We'll start our discussion by going over how machine learning algorithms can learn from data. We'll then discuss the significance and importance and the role of data in machine learning. We'll discuss what features and labels mean in the context of machine learning. Features refer to the data that we use to train the model. Labels are predictions from our model. We'll then get a big picture understanding of the machine learning workflow and the various processes that are involved. We'll then focus our attention on the data preparation stage. This involves feature engineering to convert data to features. We'll see how feature engineering is neither art nor science, it's basically just engineering, and how it involves many techniques that fall under the broad umbrella of feature engineering. And we'll round off our discussion with a discussion of the different kinds of data that we work with when we build and train ML models. Training data, test data, and validation data.

Prerequisites and Course Outline
Before we dive into the actual course content, let's take a look at some of the prereqs that you need to have to make the most of your learning. This course assumes that you are very comfortable programming in the Python language. All of the code in this course will be written using Python 3 on Jupyter Notebooks. This course also assumes that you have some understanding of machine learning and that you have built and trained simple ML models. If you feel that you lack the prereqs for this course, here are some other courses on Pluralsight that you should watch first. Python Fundamentals will get you up and running with Python. If you want to get started with machine learning, Understanding Machine Learning is a great course for beginners. And if you want to get hands on with building and training simple machine learning models, Building Your First scikit-learn Solution is the course for you. Here is a broad outline for what we'll cover in this course. We'll first discuss the role of features in machine learning and discuss different feature engineering techniques. We'll then see how you can prepare data for machine learning. This is a hands-on module. We'll then explore feature selection techniques and apply these techniques in a hands-on manner. We'll then explore feature extraction techniques and apply these in a hands-on manner as well.

Features and Labels
Before we talk about feature engineering, let's talk about what features and labels mean when you're building and training your machine learning model. If you're taking this course, you've been introduced to the concept of machine learning. A machine learning algorithm is an algorithm that is able to learn from data. It's able to look at data and find patterns and use these patterns to make analysis. Machine learning models have the ability to work with a huge base of data and find patterns within this data. These are patterns that are not easily discoverable using, say, exploratory data analysis or visualizations. Once these patterns have been determined, machine learning algorithms can then be used for prediction. Machine learning models, once it's learned from your data, are capable of making intelligent decisions. Now machine learning is a vast field with many applications, but at its core, machine learning problems can be divided into four broad categories, the first is classification, where you use your model to classify instances, good or bad, girl or boy, cat or dog. Classification models are used to predict classes or categories. If you want to predict a continuous numeric value, you'll use a regression model. Regression analysis is what you'll do to predict, say, the mileage of an automobile, the price of a stock or a home. If you have a large corpus of data and you want to find logical groupings or patterns that exist in your data, you can apply a clustering model. Clustering model tries to bring together those data points in a single cluster, which are similar to one another. And finally, if the entities in your data have many characteristics or features and you want to find which features are important, extract latent features from your data, you'll apply dimensionality reduction. As a student of machine learning, these are the first four broad categories of machine learning techniques that you'll encounter. Let's understand what machine learning is by taking an example of classification. You want to determine whether whales are fish or mammals. Now you know that whales are members of the infraorder Cetacea, which indicates that they are mammals. But on the other hand, if you look at the characteristics of a whale, they look like fish, they swim like fish, they move with fish, they live with fish in the sea. They could be fish. Now for your classifier, you essentially want to be able to feed in the characteristics of a whale in your machine learning model, so you have an ML-based classifier which has been trained on a huge corpus of data. Once it has been trained, you'll feed in the characteristics of a whale and hope for a prediction that is correct. If it's a robust, well-trained classifier, it'll correctly identify the whale as a mammal. Now the question is, how did we get this ML-based classifier trained on a corpus of data? Well, you start off with a machine learning algorithm and you feed in a huge corpus of training data to train your machine learning algorithm. This classification algorithm will look through all of the samples present in the training data and try to extract significant patterns, and this in turn will give you an ML-based classifier. A classifier is a fully trained model, the algorithm learned from data to give you a trained model, that is your classifier. If you have a good model, you should be able to give it information and have it make predictions. The characteristics of a whale that you feed into your machine learning model, whether for training or for prediction, is referred to as the feature vector. These are the features of your instance. At the other end, the output of your model, the prediction that it makes, whether it's an output category or a continuous value, such as a stock price, is referred to as the label. Now it's quite possible that you feed into your machine learning model entirely different characteristics of a whale. You tell it that it moves like a fish, and it looks like a fish. Now in such situations, your classifier is likely to indicate that the whale is a fish, which is clearly wrong. What you fed in in your input feature vector are incorrectly specified features. And when the features that you feed into your model are not set up correctly, you're likely to get an incorrect prediction from your model. Your model is only as good as the features that you use for training. The features that you use to train your model are also referred to as x variables. X variables are the attributes that the machine learning algorithm focuses on. The characteristics of the entities on which you're training your model, or the attributes, are x variables. And every data point is a list or vector of such x variables, and this is what is together referred to as a feature vector. Thus the input to an ML algorithm is a feature vector. Feature vector, exactly the same thing as x variables, and I'll use these terms interchangeably throughout this course. The output of your machine learning model, the predictions that it makes, are referred to as y variables. The attribute that the machine learning algorithm tries to predict are called labels or y variables. Once again, I'll use the terms labels and y variables interchangeably, they mean the same thing. Another term that you can use for y variables are targets. Now based on the kind of model that you're building, the labels can be of different types. If you have categorical or discrete label values, that is typically the output of a classification algorithm. Spam or ham, true or false, a, b, c, or d, these are examples of categorical values or labels. Labels can also be numeric or continuous values. These are typically the output of regression models, such as the models that you use for price prediction. When you're working with continuous output from your machine learning model, you might call them y values or targets rather than labels because labels have a very categorical feel. Now that you understand the difference between features and labels, there is an important point that you need to remember, garbage in, garbage out. If the data that you feed into an ML model is of a poor quality, the model itself will be a poor model. Your model is only as good as your data.

The Machine Learning Workflow
What are the steps involved in building and training your machine learning model? Let's take a quick look in this clip of the machine learning workflow. Here is what the basic machine learning workflow looks like. Don't be intimidated, there are lots of processes involved here, we'll walk through each of these in detail. Once you've understood what you want to build, you first need to look at the raw data that you have available. What data do you have to work with? Is it sufficient to train your machine learning model? If not, you won't be able to proceed further. You might need to go back and seek for new data sources. Once you know that you have the data that you need, you can move on and load and store the data, get it ready for machine learning. Make sure that it's located in a database or a data warehouse where you can access the data. You'll need to set up pipelines to extract the data from where you have it stored, and once you have the data with you, you'll need to clean and prepare the data. This is the data preprocessing stage. Data in the real world cannot be used directly to train your machine learning models, it needs to be preprocessed. Need to get rid of missing values, take care of outliers. If you have non-numeric representations of data, they have to be encoded to numeric form. These three processing steps that we've just discussed here, when you go from raw data to cleaned data that you can feed into a machine learning model, can together be thought of as the process of selecting and extracting features that exist in your data. When you're a student of machine learning, your attention is mostly focused on understanding the machine learning algorithm and how it works. But these three steps are critical and time consuming steps. In the real world, they take up an inordinate amount of time. It's quite possible that you spend more time on these steps than on building your machine learning model. Once you have your data ready, the next step is to choose the right algorithm for your use case. Do you want a decision tree? Do you want to use support vector machines? Do you want to use naive Bayes or _____? The choice of algorithm is up to you and dependent on your use case. Once you've chosen an algorithm, you'll then train your model on the data that you have. This is what is referred to as fitting a model. The training process tries to find the best possible model parameters so that you can use your model for prediction. Once you have a model, you need to validate and evaluate the model to see whether it's a good one. There are many validations techniques available. You'll choose a validation method and apply the validation method to evaluate your model. You'll examine the fit of your model and then update the model if needed. Examining the fit of your model is also referred to as scoring the model. You have different metrics that you can use for different kinds of models. You have the R-squared for regression models, accuracy, precision and recall for classifiers. Once you've evaluated and scored your model, you'll check to see whether you're satisfied with the result. If you're not satisfied, you might want to update the model. Maybe you'll choose a different algorithm, maybe you'll use more data for training, maybe you'll train for longer. And this is an iterative process that continues until you're satisfied with the model that you have. Update the model, choose a validation method, exam the model, evaluate it, see whether you're satisfied, and repeat until you are done. Once you're satisfied your model is ready to be deployed in production and to be used for predictions, you'll use your model for predictions, and these prediction instances are new data points that you can then store in your database to improve your model. In the real world, prediction instances often become part of the training data for your model. This is the basic machine learning workflow, and in this course, we'll focus our attention on feature selection and extraction, the initial few steps.

Components of Feature Engineering
All of our discussions so far should have emphasized to you the importance of the features that you use to train y our machine learning models. When we spoke of the basic machine learning workflow in the previous clip, we discussed the importance of the first three steps, selecting and extracting the right features to train your model. These are onerous and time consuming, and all of these encompass feature engineering. What exactly is feature engineering? Well, feature engineering basically involves working with your features, engineering your features so that you get the best out of your machine learning model. And this can be anything. Feature engineering isn't just one technique or a set of techniques or a class of techniques, it's block and tackle work where you're iteratively working and improving the features of your model. Also, feature engineering tends to be bespoke, it's specific to the problem that you're working on and the data that you have to work with. There are no set of techniques that apply to all classifiers or regressors. You can't say that this kind of feature engineering works best for neural networks, whereas this other kind of feature engineering works for traditional ML models. The idea and principles of feature engineering lie along a continuum between art and science. It's not quite art, it's not quite science, it's more just engineering where you basically bring your features together in a form that builds robust models. There is no one size fits all. Imagine feature engineering as a very broad umbrella which encompasses a number of different techniques. This involves feature selection, using techniques to select the most relevant features for your model. Feature engineering also encompasses feature learning where you can use supervised or unsupervised techniques to learn latent features that exist in your data. Feature engineering includes feature extraction as well. Feature extraction involves the transformation or reorientation of your input features into fundamentally transformed derived features, which are often unrecognizable and cannot be interpreted. Feature engineering also includes feature combination. You might have raw granular features in your data, you might combine these features together to get a feature that is more meaningful and has more predictive power. And finally, feature engineering includes dimensionality reduction as well. Dimensionality reduction involves reducing the complexity of your input data. It also involves reorienting your features along new axes, which better represent your data.

Feature Selection, Feature Learning, and Feature Extraction
Let's dive into a little more detail on each of these components of feature engineering, starting with feature selection. Feature selection involves choosing the best subset from within an existing set of features, or x-variables, without substantially transforming or changing the features in any manner. When you're building and training your machine learning model, when would you use feature selection? Let's say you have many x-variables present in your data and not all of these x-variables contain information. During exploratory data analysis, you've found that most of your features, or x-variables, contain little information, they are not really relevant to your problem. But there are a few features, or x-variables, that are very meaningful and have high predictive power, and you find that these meaningful variables are independent of each other. You will then use feature selection to only choose those meaningful variables to train your model. Feature selection techniques can be divided into three broad categories. The first of these are filter methods where you apply a statistical technique to extract the most relevant features. You can use embedded methods where you build a machine learning model that assigns importance to the different features and select those features that are the most important. And the third technique that you can apply is wrapper methods. Wrapper methods lie somewhere between filter methods and embedded methods. Here you train a number of different candidate models on a subset of features and choose that subset of features that produces the best model. Filter methods for feature selection are where you apply statistical techniques to select those features that are the most relevant. Feature selection is completely independent of actually building and training the model. With filter methods, you'll use techniques such as Chi-Square analysis, ANOVA analysis, or mutual information to determine what features are relevant to the target that you're trying to predict. Embedded methods for feature selection involve training a machine learning model. Now not all models assign importance measures to features. Relevant features can be selected by training a machine learning algorithm, which under the hood will sort features by importance and assign an importance or relevant score to each feature. Examples of such models are lasso regression and decision trees. Feature selection using embedded methods are embedded within the model training phase. That's why they are called embedded methods. When you perform feature selection using wrapper methods, you build a number of different candidate models, and each of these candidate models are built on different subsets of your features. You'll then choose that feature subset which gives you the best model. Let's go back to the different components of feature engineering that we were discussing earlier and let's move on to discussing feature learning. Feature learning is where you rely on machine learning algorithms rather than human experts to learn the best representations of complex data. This is especially useful when you're working with data such as images or videos. Feature learning is also often referred to as representation learning. Now you can have feature learning techniques that are supervised in nature when you're working with a labeled corpus of data. Neural networks are a classic example of supervised feature learning. When you feed data into neural networks, you typically don't highlight significant features, you just feed all of the data in and neural networks find out what latent features are important or significant. Supervised feature learning is an extremely important technique because it greatly reduces the need for expert judgment. When the selection of your features rely on humans, that technique will simply not scale. It's much better to have an automated solution, such as supervised feature learning. And the fact that neural networks can learn significant features in your data is an important difference between neural network and deep learning techniques and traditional machine learning based systems. Traditional ML systems rely on experts to decide what features to pay attention to. On the other hand, representation ML-based systems figure out by themselves what features are important or significant. Neural networks are examples of representation ML-based systems. One thing you'll see when you work in the real world, that it's really hard to get a labeled corpus. Most of the data out in the real world tends to be unlabeled, and thankfully there are feature learning techniques that can work with an unlabeled corpus as well. These techniques are referred to as unsupervised feature learning. Unsupervised indicating the absence of labeled data. In order to learn patterns in unlabeled data, you can apply clustering techniques. Clustering will allow you to find logical groupings that exist in your data. If you're working with image data, an unsupervised technique for feature learning is dictionary learning. Dictionary learning learns sparse representations of dense features, such as with images. If you're working with deep learning, specifically neural networks, you can use auto encoders to extract latent significant representations of your data. Now you might have machine learning models that can't work directly with raw features, which work better with derived features. And that's where feature extract comes in. feature extract defers from feature selection that we discussed earlier in that the input features are fundamentally transformed into derived features. The derived features are often unrecognizable and may be hard to interpret. These derived features might represent patterns that exist in your data, which are not intuitively understandable. Feature extraction techniques exist for data of all kinds. If you're working with images, you can use key points and descriptors to represent interesting areas in your image. If you're working with simple numeric data, feature extraction might involve reorienting your data to be protected along new axes, such as what we do when we find the principle components for matrices. Or if you're working with text data and natural language processing, you might choose to represent the words within your document using their Tf-Idf scores. This stands for term frequency inverse document frequency, and this is a score that represents how significant a particular word is in a particular document and across the entire corpus. Now it so happens that when you perform feature extraction, this also leads to dimensionality reduction. It reduces the number of dimensions which are required to express your data. So many of the feature extraction techniques that we discussed earlier also happen to be dimensionality reduction techniques. However, when you use these techniques for feature extraction, the explicit objective is to re-express your feature in a better form, not to reduce the number of X columns or features.

Feature Combination and Dimensionality Reduction
In this clip, let's first move on to discussing feature combination. Now it's quite possible that the raw data that you're working with contains very granular information, which doesn't have much predictive power. Feature combination may involve aggregating and bringing features together to get a feature with more predictive power. Now you might find that in the real world some features naturally work better when they're considered together. A feature by itself may not contain much information, but when considered in conjunction with another feature, the features in combination might contain information that is relevant to your model. It's quite possible that the original feature might be too raw or too granular. Bringing features together can help improve the predictive power of features. Let's say you're building a machine learning model to predict traffic patterns in a city, let's say the city is Bangalore. Now you might get information from the day of the week that it is, you might also get information from the time of day. But when taken in conjunction, when you use a feature cross, day of the week plus time of the day, you might get a resulting feature that has more predictive power. If you're looking at traffic at Friday evening at 6:00 PM, you know it's going to be terrible. But if you're looking at the same time, 6:00 PM on a Sunday, traffic is quite likely not as bad. Let's say you want to combine features together to predict temperature. You can take into account the current season, whether it's spring, summer, fall, or winter, you can also take into account the time of day. But when taken together, you might find that the feature combination is more than the sum of the parts. And finally, let's move on to the last component that we'll discuss within feature engineering, that is dimensionality reduction. When you're working with data in the real world, you'll find that a common problem to have is that you have too much data. This is a curse and not a blessing and is often referred to as the curse of dimensionality. This is where you'd apply pre-processing algorithms to reduce the complexity of raw features. And the specific aim of these algorithms is to reduce the number of input features so you have fewer features to work with. Having too many features to work with in your data is referred to as the curse of dimensionality, and it leads to several problems. You have problems visualizing your data, you encounter problems during training, as well as during prediction. When you work with higher dimensionality data, machine learning models find it hard to find patterns within your data, leading to poor quality models, overfitted models. Overfitted models are those that perform well in training, but poorly in the real world, in production. Dimensionality reduction explicitly aims to solve the curse of dimensionality, while preserving as much information as possible from the underlying features. You don't want to lose too much information. Dimensionality reduction is a form of unsupervised learning. You're working with an unlabeled corpus of data. Based on the kind of data that you're working with, there are many different techniques that you can use for dimensionality reduction. When you're working with linear data, you can choose principle components analysis, which involves reorienting your original data so that they are projected along newer better axes. If you're working with nonlinear data, you can apply manifold learning techniques. This involves unrolling complex forms of data in a higher axis to express data in a simpler form with lower dimensionality. Manifold learning techniques are similar to unrolling a carpet so that it's represented in two dimensions. Latent semantic analysis is a topic modeling and dimensionality reduction technique that you can use to work with text data. If you're working with images, autoencoding can find efficient lower dimensionality representation for your images.

Training, Validation, and Test Data
When you're building machine learning models, it's common practice to split up the data that you have to work with into two, maybe even three, subsets. Training, test, and validation data. Let's talk about why we do this and how this fits into the machine learning workflow. We know the basic steps involved in getting your machine learning model to production. Observe this step here where you train your model, choose a validation method, validate your model, exam and score your model, and iterate this process until you're satisfied with how your model performs. When we talk of splitting our data into subsets, these are the steps that will use those subsets of data to validate and evaluate our model. Let's say this is all the data that we have available in the world to feed into our model. This is our entire world of data. Now let's say you were take the view that you want as much data as possible to train your model. You'll take all of this data that you have, preprocess it in the right way, and feed it into build your machine learning model. But there is a problem with that. Data that you use to train a model cannot be used to evaluate a model. Once you've trained your model, you want to know whether it's a good one. You can't use the same training data for that. That's because the model has seen all of the instances in the training data during the process of training. So it's quite possible that if you evaluate your model on the training data, the model gives you a great score, but that may not mean that it's a good model. The model may have memorized training instances, and if you were to indeed use this model on instances that it hasn't encountered before, it would perform poorly. It may have overfit on the training data. So model robustness cannot be measured on instances the model has encountered before. So if you have all of the data in the world and you use everything to train your model, well, then you're left with nothing to evaluate your model. So what do you do now? So you have all of this data available to you, you split it into two subsets, training data and test data. It's quite common to use 80% of the data to train the model. This is referred to as training data and this is the only set of instances that your model will see in the training process. You set aside a portion of the original data, let's say it's 20%, to sanity check or measure the performance of your model. During the training process, your model will never encounter the test data. Which means if you want to evaluate whether your model is a robust one, that it works well on instances it hasn't seen before, well, you'll use the test data for that. You will run through one training process to generate one candidate model. You'll set up a model with a certain design and you'll get one candidate model at the end of the training process. If you want N different candidate models, you'll need to run N different training processes. And for each of these N candidate models, you'll run N test processes to evaluate these models and then you'll pick the best one. So far this is seeming like a pretty good idea, you have training data and you have test data. The test data, the test set can be used to choose the best candidate model. Which means you're evaluating a model on instances the model hasn't seen before. It hasn't seen during the training process. But there is a problem here as well. It's quite possible that your evaluation itself can become biased. When you use the same set of instances, that is the test set, to pick the best candidate model, this can lead to a kind of overfitting. This is referred to as overfitting on the test set. When you have data split into just two sets, training and test, your candidate model can end up overfitting on the test set, and once again you haven't really got a robust model here. Which leads us to the next option, cross validation. Instead of using just one test set to evaluate your model, you'll carve out a separate validation set of data points. Now you'll generate different candidate models using the training set. You'll then use the validation set to pick the best candidate model and then the final evaluation of that model will be on the test set. Let's take a look at how this works visually. You have all of the data that you're working with in the world. You'll split it into three subsets. Most of this will be training data that you'll use to train the model. You'll have two other subsets, validation data and test data. So you're holding out two subsets from the original training process. So let's say you want to build a number of different models and then choose the best one, you'll use the training data to produce the different candidate models. You'll then use the validation data to evaluate all of these models. Once you've figured out which model is the best one based on the validation data, you'll then finally evaluate that candidate model on the test data. The test data is comprised of instances that your model has never seen before, either during training or during validation. Test data gives you an unbiased evaluation of your model. And with this setup, you can see whether your final model is a robust one. You can also generate multiple candidate models and select the best one. This process is called hyperparameter tuning, and you can evaluate the final candidate model on your test data. So you'll run N training processes to generate N candidate models, N validation processes and 1 test process.

K-fold Cross Validation
So here is where we are at. We know that we need to split our data into training, validation, and test data. We'll produce N candidate models by running N training and validation processes, but just one test process to evaluate the final model that we have chosen. This is referred to as singular cross-validation. We've split our original data into training, test, and a single validation set. Let's visually see how we use these three subsets of our data to get the best possible model. We train the different candidate models on the training data, evaluate them on the validation data. This process is called hyperparameter tuning. Each candidate model will have different design parameters, you are trying to figure out which design of your model works well for your data. And finally, after you've used hyperparameter tuning to find the best design for you model, you'll do a final evaluation of the test data so you know this is how your model performs. The use of a holdout validation set is a huge improvement over what we were doing earlier. However, there is still a problem. The model's performance on the validation set gets incorporated into the model itself, and this may introduce bias. So the validation set data become part of the model's design and that's not good. What we're trying to get is a model that is as robust as we can make it, which is why an alternative to using singular cross-validation is K-fold cross-validation. Here you don't have a single set of validation data, to generate each candidate model, you'll repeatedly train and validate using different subsets of training data. Now this might not seem intuitive to you at first, but we'll see it visually and you'll understand what's going on. K-fold cross-validation tends to be very computationally intensive, but very robust. It does not waste data, all of the data is used well to generate a good model. Let's visually understand how k-fold cross-validation works. You have all of the data available to you in the real world, you split it into training data and test data. Test data is what you'll use to perform a final evaluation on the model. Now instead of using the same validation data to evaluate different candidate models, you'll split your training data into different folds. Here I have five folds, this is five-fold cross-validation. With five-fold cross-validation, for each candidate model you'll train your model five times. The first time fold 2, 3, 4, and 5 will be the training data. Fold 1 will be the validation data. You'll then train the same candidate model with a different subset of training data, fold 1, 3, 4, and 5 comprise the training data. Fold 2 is the validation data. You'll then do a third round of training for the same candidate model, this time fold 3 is the validation data, the remanding folds make up your training data. And you'll continue this for split 4 and split 5 as well. So when you use five-fold cross-validation for a single candidate model, you run five training processes and five validation processes. Training and validation is run on each fold of your training data. Once you run these five different training and validation processes, you'll average the performance of this candidate model across all folds. So you'll get one average score. And for this particular candidate model, this average performance score is what you'll use to find the best candidate model. Which candidate model has the best average performance score across all folds of training and validation. Once you've trained all of the candidate models on all of these folds and averaged their performance score, you'll take the best one that you've found and evaluate it on the test data. Thus, with k-fold cross-validation, since the validation data changes in each fold of training, it's impossible for the information in the validation data to become incorporated as part of the model.

Module Summary
And this discussion of cross-validation brings us to the very end of this module where we understood the role of features in machine learning. We started off with a discussion of the importance of data in machine learning. If you feed garbage into your model, you're going to get garbage out. Your model is only as good as your data. In this context, we discussed how machine learning algorithms can learn from data, and we studied what features and labels mean. They are also referred to as x variables and y values. We then got a big picture understanding of all of the processes involved in the common machine learning workflow. We understood exactly where in this workflow data preprocessing and feature engineering fit in. We then saw how featured engineering could mean many different things. It's a broad array of techniques, all of which fall under the feature engineering umbrella. And finally, we rounded our discussion off by talking of the kinds of data that you'll work with when you build your ML models. We spoke of the importance of training, test, and validation data, and we also discussed k-fold cross-validation to build more robust models. In the next module, we'll get hands on and we'll see how we can prepare and preprocess our data for machine learning.

Preparing Data for Machine Learning
Module Overview
Hi and welcome to this module on Preparing Data for Machine Learning. Now that you know what feature engineering is all about, we'll now discuss the real-world problems that you'll encounter when you work with data. As a student of machine learning, the datasets that you'll work with will be fairly clean, but in the real world, you'll encounter many problems. In this module, we'll understand a few data preparation techniques in detail, such as dealing with outliers and missing values. We'll also get hands on practice for these. We'll work with some real-world data. We'll read in a dataset into our program, explore it, and visualize the relationships that exist between variables, and then we'll use this data to build a simple predictive model. Any machine learning model that you build is only as good as the data that you feed in. Garbage in, garbage out. If the data that you feed into an ML model is of a poor quality, the model itself will be of a poor quality. And when you collect data from sources in the real world, you'll see that there are many issues that you have to deal with. You may just have insufficient data, not enough data to train a model. Or you may have too much data and not all of the data is relevant. You might have data, but it's non-representative, not really suitable for what you're trying to model. Or you might have data with missing values, data which contains duplicates, data that contains outliers. Dealing with each of these issues is an entire topic or a course of its own. We'll discuss some of the problems and implications in some detail.

Problems with Data
All of the processes that we discussed under the broad feature engineering umbrella are extremely important because when you work with data in the real world, you'll find that you'll encounter several problems with data. Let's dive into each of these problems in a little more detail in order to understand the impact that this can have on your machine learning models, starting with insufficient data. Now models that are trained with insufficient data perform poorly in prediction. Machine learning models need a lot of data in order to find significant patterns in the data. Strangely enough, using insufficient data to train your model can lead to the problem of overfitting your model on the data. Overfitting essentially implies that your model performs very well in the training phase, but poorly on prediction instances in the real world. This happens when your model memorizes training instances rather than extracting patterns from those instances. Insufficient data can also result in underfitted models. Underfitting is when you build overly simplistic models from the available data. The model is too simple to have understood or extracted patterns from the data. There is no real solution for this problem of insufficient data. There are techniques that you can use to mitigate this, but the real solution is get more data sources. Another problem that you might encounter when you're working on real world projects is that of too much data. Organizations love collecting data, there might just be a lot of it, not all of it relevant. Too much data can manifest itself in two ways. The curse of dimensionality is when all of your records have too many features. You have too many columns in your dataset. Not all of those features might be relevant. If you have too many features in your data, you might need to use feature selection or dimensionality reduction techniques to extract features. Or you might have too much outdated historical data. There are too many rows and the data is too old for it to be significant. Historical data tends to be a real problem when you're modeling time series, such as financial data. The only solution here is to use your judgment to determine what rows are still relevant. It's also possible that the problem that you face is that your data is non-representative. That is, your data is not representative of how things are in the real world. Remember that the data that you're working with is a sample that is drawn from a population. Now the way the sample has been drawn can lead to biases. Let's say you meant to sample the engineers across all countries in the world, but your data collection process basically lost the information for two or three countries. Well, that would immediately introduce a bias in your data because your data is not representative of the real world. And bias data leads to bias models that perform poorly in practice. There are certain techniques that you can use to mitigate this. You can oversample instances which are rare or you can under sample very common instances to get a more balance dataset. These come with their own problems though, and you need to use your judgment before you apply these techniques, apply with care. You might find that your data is riddled with errors. You have missing data and you have outliers. Well, these are some of the most common problems that you'll encounter, data in the real world is raw and needs to be cleaned and prepared before it can be fed into a model. There are a bunch of data cleaning procedures that can significantly mitigate the effect of both of these issues. And one last category of problems that you might encounter is that of duplicate data. Now duplicates might seem simple to resolve in practice. If data can be flagged as a duplicate, that problem is relatively easy to solve. You simply apply de-duplicating procedures. But in certain applications, such as streaming applications, it can be hard to identify dups in real time.

Dealing with Missing Values
While discussing problems that you encounter when working with data in the real world, we discussed a number of techniques that you can use to deal with these problems. Let's dive into a little more detail on some of these techniques, starting with missing values and outliers. When you're collecting and working with data, you might find that you have missing data in the form of missing values for fields. Or you might find that your data contains outliers which don't really make much sense. When you're working with data that is missing from the records that you have to train your model, there are two approaches that you could follow in order to deal with this data. The first of these is deletion where you get rid of data which has missing fields. The other is imputation, where you fill in missing values using sound techniques. Let's talk about out deletion first. Deletion is also referred to as listwise deletion. This is where you delete an entire record that corresponds to a row in your dataset, if you have a missing value in a feature, that is a column of your dataset. This is a simple hassle-free technique to get rid of missing values, but it can lead to bias because you're getting rid of entire records, even if an irrelevant field has a missing value. Listwise deletion is often the most common method in practice because it's easy. But this can lead to several problems. It can greatly reduce the sample size that you're working with. If you don't have very many records to begin with, if you get rid of entire records due to a few missing fields, you might get into a situation where you have insufficient data to train your machine learning model. Now there are other nuances that you have to worry about as well. If the field values are not missing at random in your dataset, there is one sensor which never has a value for a particular field. If you go ahead and drop all records from that sensor, that can lead to a significant bias. So it's pretty clear that simply dropping entire records, which have a few fields missing, is not a great option, which is why we move on to imputation where you fill in missing column values rather than deleting records. Missing values can be inferred from the data that is already available, from known data. Once you've decided that you want to use imputation to fill in missing values, there are a number of methods that you can choose from. They range from the very simple to very complex. The simplest possible method is to use the column average. You assume that the missing value is essentially equal to the mean value in that column or for that feature. Other very similar options are to use the median value of that column or the mode for that column. Another way to impute missing values is to interpolate from other nearby values. This technique is useful when your records are arranged in some kind of inherent order. Imputation to filling missing values can be arbitrarily complex, in fact, you can build an entire machine learning model to predict missing values in your data. There are other techniques that you can apply to fill in missing values as well, such as hot-deck imputation. You'll sort all of the records that you have based on a criteria that is important, and for each missing value, you can use the immediately prior available value. This is referred to as last observation carried forward. Fill in missing values using the previous value, once your records have been ordered. This is especially useful for time series data where progression and time has meaning. When you're working with time series data, this last observation carried forward is equivalent to assuming that there has been no change in this value since the last period of observation. A common technique that is often used, which is an example of univariate imputation, is for each missing value, substitute the mean of all available values. Mean substitution has the effect of weakening correlations between columns that exist in your data. When you essentially say this is an average data point, there is nothing special about it, you weaken correlations, and this can be problematic when you're performing bivariate analysis, analysis to determine the relationship between two variables. If you want to be able to intelligently predict missing values in your data, you might want to use machine learning. Fit a model to predict missing columns based on other column values. Applying this technique tends to strengthen correlations which exist in your data because you're essentially saying that this column is dependent on the other columns. Thus regression and mean substitution to fill in missing values have complementary strengths. You need to be aware of the nuances of applying these techniques and use your judgment in your specific use case.

Dealing with Outliers
As you're exploring your data and visualizing it, you might find that you have outliers in your dataset. a data point differs significantly from other data points in the same dataset. It might be that the entire record is an outlier in some manner or there are certain fields with outlier values. When dealing with outlier data, it's a two step process. The first step is to identify outliers that exist in your data, the second step is to use techniques to cope with these outliers. Just like there are machine learning algorithms, there are specific algorithms that have been built for outlier detection. But at the very basic level, you can identify outliers by seeing the distance of that data point from the mean of your data or the distance from a line that you fit on your data. Once you've identified outliers, you can cope with outliers using three broad categories of techniques. You can drop records with outlier data, you can cap and floor outliers, or set outliers to the mean value of that feature. Let's start our discussion by seeing how we can identify outliers using the two techniques that we discussed. The mean or the average of any feature of any feature in your data is basically a measure of central tendency. That is the point around which the remaining points are clustered. If you have a data point with a value far from the mean, that can be considered to be an outlier. Or you can perform some kind of regression analysis and find a line or a curve that follows the patterns in your data. And if you have a point that is far from this fitting line, that is an outlier. When you want to quickly summarize any set of data that you're working with, the first measure that you'll indicate is the mean of that data. The mean value of any data is the headline, it's the one number that represents all of the data points best. The mean of the average of any set of data points is essentially the sum of all of the numbers divided by the count of the numbers. Hopefully you remember this from high school. However, along with the mean, the variation that exists in your data is also important. The variation is a measure of whether the numbers in your dataset jump around. One important measure of the variation in your data is the range, which is simply the maximum value minus the minimum value. However, the range completely ignores the mean and is very swayed by outliers that are present in your data, which is why often another measure of variation is used, that is the variance. The variance of your data is the second most important number to summarize any set of points that you're working with. And the formula for variance is as you see here on screen. You don't really have to worry about the formula though, the variance is a measure of how your data jumps around or varies. You need to have an intuitive understanding of what mean and variance represent. They succinctly summarize a set of numbers, any set of numbers. Now along with variance, another term that you might encounter is the standard deviation. The standard deviation is nothing by the square root of variance and is a measure of variation in your data. The standard deviation helps you express how far a particular data point lies from the mean. Points that lie more than 3 standard deviations from the mean are often considered outliers. The standard deviation threshold for outliers is often based on your use case. So you'll have data that is 1 standard deviation, 2 standard deviation, 3, 4, 5 standard deviations from the mean, and you can determine the threshold for outliers. Another way to identify outliers in your data is to measure their distance from a fitted line. So you have a huge number of points and I'm going to represent this in two dimensions because that is by far the easiest. You'll try and fit a line using some kind of regression analysis on this data. So outliers are essentially data points that do not fit into the same relationship as the rest of the data. And now based on these two principles, there are a variety of algorithms that you can use to identify outliers, but once you identified these, you need to figure out how you want to deal with them, how you want to cope with them. Now here there isn't really one technique which is the one that you should use. You need to scrutinize and understand the outliers that exist in your data. Are these outliers because of incorrect observations or errors that exist in your data? You might want to get rid of that record entirely if all of the attributes of that record are erroneous. Or if for a row or a record, if you feel that there is just one attribute that has been erroneously recorded, you might want to set that outlier value to be the mean, and not drop the record as a whole. Now it's quite possible that your outlier data is not an incorrect observation. It's a genuine legitimate outlier. If your model is not distorted to the presence of such outliers, leave it as is. The outlier actually conveys important information that your model might need to recognize. Or if you don't want to leave it as is, you can cap or floor the outlier if your model is distorted. This might require that you standardize your data first, that is express all of your data points in terms of standard deviations from the mean. Standardization involves subtracting the mean from all of your values so that your resulting scaled data has a mean of 0 and all values are expressed in terms of standard deviations. Once you've done this, you can cap positive outliers to be just 3 standard deviations from the mean to +3 and you can floor negative outliers to -3. They are, once again, 3 standard deviations away from the mean.

Applying Different Techniques to Handle Missing Values
In this demo, we'll study some of the techniques that you can use to handle missing values that might exist in your data. We'll use Python to implement all of the demos in this course, and the IDE that we'll work with will be Jupyter Notebooks. Jupyter Notebooks are a browser-based shell that allow you to execute Python code right here within your browser and see the results right away. Here is our Jupyter Notebooks server, running and pointing to our current working directory. Under this we have a datasets folder. And if you click through, you'll see that it holds all of the datasets that we'll download and use for all of the demos in this course. Let's go back. This a current working directory. All of our Jupyter Notebooks will be placed within this directory. We'll write all our code using Python 3, so make sure that you use the Python 3 kernel when you execute your code here in Jupyter. Here is our first untitled notebook. I'm going to rename it to be called HandlingMissingValues. We'll use a number of Python data science specific libraries in order to wrangle and work with our data. Pandas and numpy and matplotlib to visualize our data. Set up the import statements for these models and let's take a look at the current versions that I have installed on my machine. Remember, if you're watching this course at a later date, your version might be different, and if you feel that your implementations are not exactly the same, the version mismatch would be an issue. That's why it's good to take note of these versions. In order to see how we can use different techniques to deal with missing values, I'm going to set up a handcrafted pandas data frame here, which is three columns corresponding to three different features. Two of these features have missing values and one doesn't. You can see here that feature 1, 2, and 3 are all numeric. Feature 3 has no missing values, but feature 1 and 2 do. And these missing values are represented using NaNs, or not a numbers. Pandas data frames offer a number of useful techniques to detect whether your data has missing values. Df.isnull will give you a binary representation that is true/false values for all fields that are missing. All fields that are missing will have a value of true for is null. Df.isnull gives you a data frame as a result with true/false Boolean values. If you think of true as 1 and false as 0, you can simply call sum on isnull and this will give you how many values for each of these columns are missing. You can see that feature 1 has 2 missing values, feature 2 has 3 missing values, feature 3 has no missing values. The fillna function in pandas allows you to fill in missing values using a variety of different techniques. Here I've specified the pad method here and limit is equal to 1, and let's see how the missing values have been filled in. You can see that the pad method simply takes in the value from the previous row and uses that to fill in the missing values. We specified a limit of 1 while we padded missing values, so there are 2 consecutive missing values here in feature 2. You can see that only the first one has been filled in with 300, the second missing value remains as is. This is because of our limit is equal to 1 parameter. I'm now going to invoke the pad method once again, but this time with a limit of 2. And this time, observe that both of the contiguous missing values in feature 2 have been filled in with 300, that's because of our increased limit. If the records in your data frame are arranged in some kind of intrinsic order, maybe ordered by date, or the ordering may be based on a value in a particular column. In the case of order data, another technique that you could use to fill in gaps is the backfill technique. Let's take a look at how this fills in the same values. Observe feature 1 here where the first record has a missing field. The value 3 from the next record has been backfilled to fill in the missing value in the first record. The value in the field after the missing field has been backfilled into the missing field. Observe this missing value here and NaN that hasn't been backfilled because no record is available after the last record to fill in the missing value. And here are the two contiguous missing values filled in using backfill. The last record, which comes after the last missing value, has a value of 600, and this value of 600 is used to backfill the holes. Just like the backfill or the bfill, you can use the ffill, or the forward fill, to fill in missing values. Observe that this time around, the missing value in the last record has been filled in with the value from the previous field in the data frame. It has been forward filled. But the missing value here in the first record hasn't been filled in because there is no record available before the first record to perform the forward fill. And the two contiguous missing values here in feature 2 have been forward filled using the value from the previous record, which is 300. Now it might be that in some situations you don't want to have to deal with filling in missing values at all, especially if you have sufficient data to build your model. In such situations, you can use the dropna function to drop all records with missing values. Axis is equal to 0, which is what we had specified, and our dropna function refers to records or rules. There are only two rules with no missing values, only those are preserved in the result. Dropna also allows you to drop features that have missing values, simply specify axis is equal to 1, and all features with missing values will be dropped. There is just feature 3 with no missing values, which is preserved in the result. Now dropna, the way we've used it so far, simply drops features or records if even a single field is missing. Instead of that, you might want to specify a threshold of missing values based on which you'll drop a feature or a record. The threshold that we have specified here using the thresh input argument to dropna, is that 90% of the data should be available for a particular column. Df.shape0 will give us the number of records. We multiply that by 0.9 to get 90%. We will drop those features which have more than 10% of the data missing. In our extremely tiny toy dataset here, that leaves us just with feature 3. Ninety percent for the small dataset gives us 6.3, which means even if one field is missing, that feature will be dropped. Fillna also allows us to fill in missing values using a constant, and this is commonly used to fill in the missing values of a column using the average of their column values or the median or the mode. Here we fill in the missing values in feature 1 using the average of the data. The average for feature 1 is calculated to be 9, so all of the missing fields are filled in with 9. Pandas also allows you to fill in missing values using interpolation. Interpolation is a numeric technique that you can use to reduce new data points from existing data. Invoke the interpolate function on feature 2 and observe how the existing data points have been used to deduce missing values. Value between 100 and 200 has been interpolated as 150. The two missing values between 300 and 600 have been interpolated as 400 and 500 respectively.

Detecting and Handling Outliers
In this demo, we'll explore a few techniques that you can use to handle outlier data that might exist in your dataset. We'll write the code for this demo in a brand new Jupyter Notebook called HandlingOutliers. Instead of generating our own data with outliers, I'm going to use the mall_customers dataset that's available here at this URL, that's the original source. I have this dataset stored as a csv file on my local machine and I'm going to read that into a pandas data frame. This dataset contains information on customers who visited a mall. We've got the CustomerID, the gender, male or female, the annual income of the customer, and a spending score in the range 1 to 100. This is not a very large dataset. There are just around 200 records. This dataset is a clean dataset, it has no missing values, and a quick way to check this is to invoke the isnull function and then the any function. You can see that there are no null values at all. Other techniques that you can use to explore your data is to see the number of unique values present in each column using the nunique function. You can see that the CustomerIDs are all unique. There are 2 genders, male and female, 50 unique values for the age, and 67 unique values for the annual income. One technique that you could use to see whether any of the columns contain outlier data is to plot a histogram representation of the data. The pandas library in Python uses matplotlib under the hood in order to display these visualizations use plot.hist. I have 10 bins in my histogram. And you can see from this distribution here that most of the data, that is most of the annual incomes lie in the range 10 to around 125K per year, expect for a few data points which are at the extreme right. This can be considered to be outlier data. If you want to visualize bivariate relationships that exist in your data, that is relationships between two variables, you can use the scatter plot visualization. Here I've plotted a scatter plot of age versus annual income. Let's take a look at what this plot looks like, and you can see that there are two data points here at the very top close to $175, 000 per year annual income. Those are likely to be the points that we visualized in our histogram representation. Let's continue our exploration of this data. Let's plot age versus annual income, but we'll also color all of the data points by spending score. Observe that c, which is color, is equal to spending score. The spending score of the individual will determine the color of the data point. And here is what the resulting visualization looks like. Observe that we have a color scale to the extreme right of our graphical representation and this color scale shows us the range of the spending scores. A high spending score is denoted using lighter yellowish points. A very useful visualization for outlier data is the box plot representation of your data. A box plot gives you an idea of where the medium of your data lies and also denotes outliers. Observe that each of our columns are represented using a box and whiskers, and the points that you see highlighted are the outliers in annual income. The box here represents the interquartile range, that is data points that lie between the 25th percentile and the 75th percentile. It's pretty clear from all of our data exploration that the only outliers exist in the annual income field. So I'm going to explore the box plot visualization for only the annual_income column. And here at the outliers that we saw earlier. The whiskers in this box plot representation, that is the horizontal lines that you see above and below the box, represent 1.5 times the interquartile range from the 25th percentile and the 75th percentile of your data. We now know that the outlier data in the annual_income field are customers who have an income of greater than $125, 000 per year. So let's filter our data frame to find those records. Here are customers at indices 198 and 199. We identified these outliers by exploring the data using different visualization techniques and identifying a cutoff for the annual income and then figuring out what the data points were. Based on your use case, what you define as an outlier might be different. For example, here I'm going to calculate the value of Q1 and Q3, that is the data point that is at the 25th and 75th percentile of my annual income. The 25th percentile is an annual income of 40, 000, at the 75th percentile is an annual income of roughly 77, 000. Now if you remember your high school statistics, which I do kind of vaguely, the interquartile range is the difference between the 75th and 25th percentile, Q3 minus Q1. And I'm going to calculate and store this value here in the IQR variable. I'm going to use this interquartile range in order to identify outliers that might exist in my data. I'm going to add a new column to my data frame first called Annual_Income_Outlier and initialize it to false initially. Remember that the technique that you use to identify outliers can be different based on your use case. Here I'm going to say any data point that lies 1.5 times the interquartile range away from Q1, that is the 25th percentile on the upper end, that is an outlier. That is any record having an annual income of greater than 96K roughly, I consider outlier data. Once I've defined what my outlier cutoff is, all I need to do is run a for loop through my data, go through every record, see whether the annual income lies above this cutoff, and if it does, I'm going to set the value in the Annual_Income_Outlier column to be true. And if you sum up the values in that column, you'll see how many outlier points that you've detected using this technique, and it is 20. I don't have very much data to work with, so I don't want to drop the records that I consider to be outliers. Instead I'll first identify all of those records where the annual income is not an outlier, where Annual_Income_Outlier is set to false. I now have all of the non_outliers records here in this non_outliers data frame. I'll now use the non_outliers to calculate the mean value for annual income. And I'll use this mean value to fill in the values for all of the outlier data. The mean here is 54. Filling in the outlier values with the mean requires another for loop. Simply iterate over all of the records in your pandas data frame, and if it's marked as an outlier, fill in the value in the Annual_Income column to be equal to the mean of the non_outlier data. All of the points that we consider to be outliers in annual income have been filled in with the mean values of the non_outlier data. And if you take a look at the box plot representation of the Annual_Income column now, you'll see that there are no outliers.

Reading and Exploring the Dataset
In this demo, we'll read, explore, and visualize a dataset from the real world. We'll then use this data to build and train a regression model, which we'll use for price prediction. Here we are in a brand new Jupyter Notebook and the dataset that we'll use for this demo is the cars dataset, the original of which is available at this link here on GitHub. I've downloaded a slightly modified and cleaned up version of this dataset onto my local machine and that's what I read into my data frame. As you can see here, this dataset has a large number of features. There are 27 columns, 26 features, and 1 column that is the price, that's going to be the target of our regression model. The price is what we'll try to predict from the other features of the cars. A number of features here are categorical in nature, such as the manufacturer of the car, the model of the car, the type of car, small, medium, compact, or large. There are other continuous values as well, such as the luggage room, weight of the car, and so on. If you want to know what data is available for each car, you can take a look at the columns attribute in your data frame. And here are all of the columns of data available, which we can use to train our regression model. This dataset is fairly small, just 92 records. In the real world, it's hard to work with such little data to build a good model, but it's enough for our prototype. Let's move on and let's take a look at additional information that we have on the various features. The info function will give you the type of each column and whether it contains null values or not. You can see that many of these columns are numeric, containing integers or floats, others are objects. These are columns with categorical or discrete values. Another function that you can use in your pandas data frame is the describe function that gives you a quick statistical overview of all of the numerical features present in your data, such as the count, that is the number of records, mean standard deviation, and the quartiles. If you have categorical values in your data, one way to see the unique categories that are present is to use the np.unique function. Here are the unique values that are present in the Manufacturer column. If you have a dataset with a huge number of features, it would be helpful to zoom in on a few columns to explore your data. Here I've filtered out to view only the columns Manufacturer, Model, and MPG.city. Within square brackets I've simply specified the list of columns that I'm interested in, and here is the data in those columns, just a sample. Pandas also allows you to filter data within your data frames based on the condition. Here I want to view all of the cars that cost under 35K. I simply specify price less than equal to 35 and I get the result in another data frame. There are 86 cars in our dataset in this price range, under $35, 000. Every record in your pandas data frame has a unique index and typically these indices are integer values. You can use the iloc function in order to access data at specific indices. Here are the records with the index values 12, 19, and 47. I'll now use the isnull.any function in order to check whether there are any columns that have missing values. And down here at the bottom, I see that there are two columns, the rear.seat .room and the luggage.room. Now let's see how many records have missing values in these columns, call isnull and invoke the sum function. And this shows me that there are 11 records with luggage.room missing and 2 records with the rear.seat .room missing. I'm going to explore these missing values further by assigning these records to another data frame called cars_data_nulls, and I'm going to visualize these four columns of data. You can see that a bunch of luggage room information is missing. Now you can, of course, use any of the techniques that we've discussed earlier to fill in these missing values. You can fill in these missing values using the mean of your data, if you arrange your data in some specific way, you can fill in using forward fill and back fill. Or if you feel that you have sufficient data to work with, we can simply drop the records with missing values. Let's see the indices of our records first. These are all of the records with missing values. I'm going to go ahead and drop all records with any fields missing, and I'm left with 81 records in my cars data. We have a large number of features, 26 columns of features, and I'm going to select just a few of them in order to build my regression models. And I'm going to store these columns in the selected_columns variable and use only those selected columns. So we've pruned the dataset that we're working with to have 81 records and 8 columns of data, 7 of which are features, 1 is the price, that's what we'll predict. If you take a look at the sample of this data frame, here are the features that we are left with, and this is what we'll explore further using visualization techniques. I'll first plot a scatter plot to see the relationship that exists between the size of an engine and the horsepower of a car. A scatter plot is an extremely useful technique to visualize bivariate relationships in your data, and you can see that there is a clear linear relationship here. As the engine increases, horsepower tends to increase. There is one categorical feature in the features that we've selected, that is the Origin column. And these are cars made in the USA or outside of the USA. Machine learning models cannot work with categorical strings, so we need to convert this data to numeric form. And for that, we'll use the LabelEncoder available in scikit learn. When you use this label encoder and call fit_transform on your categorical data, it will assign unique numeric integer identifiers to the categories that exist in your data. Let's take a look at the origin field once it has been label encoded, and you can see that the origin is now encoded using 0, 1 integers. If you use the label encoder for categorical data, which has more than two categories, remember that these unique integer identifiers will be considered to be ordered by your machine learning algorithms, that is a value of 3 will be considered greater than a value of 1. If there is no inherent ordering to your categories in your categorical data, then label encoder may not be the best choice. Now let's go ahead and see other categorical features that are present, that is all columns which have the data type object. Of the features that we've selected for our regression model, there are two other categorical features, the Manufacturer and the DriveTrain. We've seen the unique values in the Manufacturer column, let's do the same for DriveTrain, and you can see that there are three options here, Front, 4WD, and Rear wheel drive. When the categorical values in these columns have no inherent ordering, that is rear wheel drive is, say, no better than front wheel drive, you can one hot encode your categorical data, and you can do this using the pd.get_dummies function. One hot encoding will represent each categorical variable using as many columns as exist categories in your data. And a value of 1 in a column, for example Manufacturer_Subaru here, indicates that the manufacturer is Subaru. All other columns for this categorical variable will be equal to 0. Let's scroll to the very right here and here are the three columns representing the DriveTrain category, 4WD, Front wheel drive, or Rear wheel drive. One hot encoding is a commonly used technique to convert your categorical variables to numeric form, and it'll increase the number of features that you have to deal with because a new feature or column has been added for each categorical value in your categorical variable.

Perform Simple and Multiple Linear Regression
Now that we've filtered, explored, and preprocessed our data, we are ready to fit our linear regression model. And we'll use the linear regression estimator object from scikit learn. We'll first perform simple regression using a single feature. We'll use the horsepower of a car to determine its price. Store the horsepower in the X variable and the price in the Y variable. We'll then use the train, test, and split function in scikit learn to split our data into a training subset and a test subset. The training data is what we'll use to train our regression model, the test data is what we'll use to evaluate our regression model. The test data is data that our model will not see during training. Twenty percent of the data we keep aside to test our model. We've seen before that our dataset here is fairly small. We'll use 64 records to train our model and 17 records to evaluate our model. The higher level estimator APIs in scikit learn make it very easy to build and train regression models, or any kind of model. Simply instantiate the estimator object and call fit on your training data. And once the training is complete, we can call score on our linear model to evaluate our model on the training data. Observe that I've passed in the training data here, and the score that we get here is 0.64. This score is the R-squared score of our regression model, and the R-squared score is a measure of how much of the variance in the underlying data has been captured by the straight line that we fit on our data points. And you can see that it's 0.64 or 64%. Now we want to see how this model performs on test data. We'll use this model for prediction, called predict on X_test, and here are the predicted values from our model. In order to observe actual versus predicted values side by side, I'm going to set these up in a single data frame. The actual values are in the test column, these are from our dataset, and the predicted values are from our model. And given that we just used a single feature, that is horsepower, to predict the price of these cars, you can see that our model did fairly well. But the real test is the R-squared score. Calculating the R-squared score in the test data is an objective measure of how good our model is, and you can see that the R-squared score here is 0.74, better than on the training data where it was around 0.64. This is clearly a robust model. Since we used just a single feature to train our model, we can visualize the actual data in the form of a scatter plot and fit the predicted values in the form of a line using matplotlib. And here is what our linear regression model looks like. The red straight line is the model that we fit on the underlying data, and you can see that it matches pretty closely. Now instead of using just a single feature in our regression model, we have many more features available, let's use all of them. So I'm going to go ahead and use the Price column and assign it to the Y variable. That is our target value that we are trying to predict, and all of the remaining columns are the features that I'll use to train our model. And if you take a look at the sample of our X data, here are all of the features that we'll use to train our model, including the one hot encoded categorical values. Let's go ahead and split our data into the training subset and the test subset. Once again, we'll use 64 records to train our model. Many more features though. And 17 records to evaluate our model. Instantiate the LinearRegression estimator object and call fit on the training data, and once a model has been trained, let's use it for prediction. Once we have the predicted values from our model, we can use the R-squared score to evaluate our model. Let's first calculate the R-squared value on the training data, and this time it's 0.96. Now this seems like a great R-squared score, and it is. But it's quite possible, given the small number of records that we use to train our model, that our model has overfit or memorized the training data. Let's evaluate our model by invoking the score function on our test data. You can see that the R-squared here is very low, 0.53. This is clearly not a very good model, even though we increased the number of features used to train it. A high R-squared in training and a low R-squared on test data indicates an overfitted model. Let's calculate the same R-squared on our test data using the r2_score function, and you get the same value. This clearly is not a great model.

Module Summary
And with this demo, we come to the very end of this module on preparing data for machine learning. We started this module off with a discussion of problems that you might encounter when you're working with data in the real world. You may not have enough data. You may have too much data, your data might be nonrepresentative. We discussed each of these problems in a little bit of detail and discussed a few approaches that we could use to mitigate these issues. We then discussed in a lot of detail how we could deal with outliers and missing values in our data, and we also applied these concepts in practice. Once we understood how to deal with missing values and outliers, we moved on to reading in a dataset and performing exploratory data analysis. We visualized the relationships that existed in our data and we then built a simple predictive model. Our model was a regression model to predict continuous values. We used it for price prediction. In the next module, we'll dive into more detail into one of the components of feature engineering. We'll see how we can use feature selection techniques to select relevant features to train our model.

Understanding and Implementing Feature Selection
Module Overview
Hi, and welcome to this module on Understanding and Implementing Feature Selection. In an earlier module, we discussed a number of techniques that fall under the broad umbrella of feature engineering. Amongst these was feature selection, and this actually becomes very important because if you train your model with irrelevant features, you won't get a good model. We'll start off by understanding what exactly feature selection is. We'll get a broad understanding of when you would choose to use feature selection and how it differs from dimensionality reduction. We'll then move on to discussing and implementing specific feature selection techniques in detail, starting off with filter methods. This is where you apply statistical tests to find which features are the most significant. We'll understand the theory behind filter methods. We'll also apply it in practice. And the same goes for embedded methods of feature selection. Embedded methods are where you train machine learning models and those models give you which features are important. Not all machine learning models support this technique for feature selection, decision trees and lasso regression do. We'll then move on to wrapper methods for feature selection. We'll understand what they mean and also apply it in practice. Wrapper methods also involve the use of machine learning models to find the best feature subset. And along the way, somewhere in there, we'll discuss the types of data that you'll work with in ML models, numeric data and categorical data, and we'll discuss the different measures of correlation that you can use to understand the relationships that exist in your data.

Types of Data
When you're working with features to build and train your machine learning models, you'll find that your features or data can be divided into two broad categories. There are two types of data that you'll encounter in machine learning, numeric data and categorical data, and each of these categories can be further subdivided. Numeric data is continuous data. It can take on any value from a continuous range. Height of an individual, weight of an individual, stock price, all of these are examples of numeric data. Categorical data, on the other hand, can only take on discrete values, male or female, true or false, dog, cat, or bird. Numeric data can be further subdivided into two subcategories, ratio scale data and interval scale data. Categorical data also has two subcategories, ordinal data and nominal data. Now numeric data has an intrinsic order and numeric data can always be sorted on the magnitude. Ratio scale data within the numeric category is the usual numeric data, one that we are familiar with. Any number that can be expressed in the form of a ratio. All arithmetic operations, addition, subtraction, multiplication, and division, apply to ratio scale data. The magnitude of ratio scale data can also be expressed in terms of multiples. For example, a weight of 20 pounds is twice as much as a weight of 10 pounds. An important significant point that makes ratio scale data unique, as compared with other kinds of data that we'll discuss here in this chart, is that ratio scale data has a meaningful zero point. This is the only type of data in this chart that does. You can meaningful say that a weight of 0 pounds or a height of 0 pounds or 0 price is equivalent to no weight, no height, or free. On the other hand, interval scale data is still numeric data in that it can take on continuous values. But these are ordered units that have the same difference or interval. Number of visitors to a site per hour, the number of heads per hundred coin flips, these are examples of interval scale data. Now with interval scale data, the data is still numeric, but certain operations may not make sense, such as multiplication and division, and there is no meaningful zero point. An example of interval scale data is, say, temperature. You know that the difference between 90 Fahrenheit and 30 Fahrenheit is equal to 60 Fahrenheit. So differences are meaningful here. But a temperature of 90 Fahrenheit is not thrice the temperature of 30 Fahrenheit. Also, 0 doesn't really make sense here. Zero Fahrenheit is not equivalent to no temperature. The main thing about all kinds of numeric data is the characteristic that I mentioned earlier, it can draw from an unrestricted range of continuous values. Numeric data is typically predicted using regression models, and numeric data lend themselves to all kinds of descriptive statistics, mean, mode, median, standard deviation, Pearson's correlation, etc. Let's move on from numeric data to categorical data. Let's talk about ordinal categorical data first. Ordinal data can only take on a discrete set of values from a restricted range. Ordinal data is still categorical, but has some kind of intrinsic order or ranking. Categorical data that can be ordered is ordinal data. Month of the year, ratings on a scale of 1 to 5. You know that a 5 star rating is better than a 1 star rating. You know that July comes after March. There is an inherent order to ordinal data. Order exists, but the differences are not necessarily meaningful. For example, the differences in quality between three, two, one, and no Michelin stars for a restaurant are not the same. The differences between two categories in ordinal data need not be uniform. Nominal categorical data, on the other hand, is data that takes on values from a restricted range, but there is no inherent ordering or ranking to the categories. All of the cities in the United States, that's an example of nominal data. It's a restricted set, it's categorical, but there is no inherent order. Other examples of nominal categorical data are male and female classification, cat or dog. So how would you remember the differences between the two? Ordinal data can at least be ordered, nominal data are simply names. Categorical data, whether ordinal or nominal, are predicated using classification models.

Measuring Correlations
A part of understanding your data and before you apply feature selection techniques, is to understand the relationships that exist in your data. And a technique to measure the strength of a bivariant relationship, that is a relationship between two variables, is the correlation. A correlation is any statistical relationship, whether causal or not, between two random variables. It's important to remember here that correlation is not equal to causation. Correlation only tells us that a statistical relationship exists, and it may tell us the direction of that relationship. The correlation coefficient is the numerical measure of the correlation between two random variables. The correlation coefficient is typically a number between -1 and 1 where positive numbers indicate a positive relationship, negative numbers indicate a negative relationship, and zero indicates that the two variables are independent. Based on the kind of data that you have, there are different techniques that you can use to measure correlation. Pearson's correlation, Kendall's correlation, and Spearman's correlation are the most common amongst these techniques. If someone uses the term correlation without specifying what correlation, they're typically referring to Pearson's correlation. That's what is used the most often. Pearson's correlation is the most restrictive and the most common correlation measure. It works only with numeric data, both ratio scale and interval scale. It does not work with ordinal data. Kendall's correlation is more general purpose, it's a rank correlation measure, that is, it takes into account the difference in ranks between your variables. It works with interval scale data, ratio scale data, as well as ordinal data. And the third kind of correlation, Spearman's correlation, is also a rank correlation measure and works with interval scale, ratio scale, as well as ordinal data. When you calculate the correlations between random variables using any of these three techniques, you'll get a correlation coefficient. The correlation coefficient will have a maximum value of +1. Two variables are said to be perfectly positively correlated when the correlation coefficient is +1. A correlation coefficient of +1 between two variables, X and Y, indicate that if X changes in one direction, Y will also change by an equal amount in the same direction. The minimum value for a correlation coefficient is -1. Two variables, if they have a correlation coefficient of -1, are said to be perfectly negatively corelated. Minus 1 indicates the directionality of them all. If X increases, Y decreases by a proportionate amount. Uncorrelated data has a correlation coefficient of 0. Now that we know the shared metrics, let's discuss each kind of correlation in turn. Pearson's correlation coefficient is the commonest metric that you'll see out there. And it's a measure of the linear relationship that exists between two random variables. Pearson's correlation can be measured and has meaning only when both of the random variables are numeric, either ratio scale or interval scale. And finally, Pearson's correlation makes the assumption that your random variables are normally distributed. Let's move on to the rank correlation measures, starting with the Kendall rank correlation. Rank correlation measures the similarity or differences in the ranks of your variables. This means that Kendall's correlation will even work with ordinal data because ordinal data has an inherent ranking or ordering. Of course, none of these correlation measures work with nominal data, it doesn't make sense there. Kendall's rank correlation is used to measure whether ranked orderings are similar or not. And it does not assume a linear relationship between variables. And the third correlation measure, Spearman's rank correlation, will work with ordinal data, exactly like Kendall's correlation. This makes no assumption of a linear relationship between variables, but it assumes a monotonic relationship, relationship that is either increasing or decreasing. So how would you evaluate correlations after you calculate the correlation coefficients? When would you say there is a strong correlation between variables? There is Cohen's standard to measure the strength of association. For a correlation coefficient between the absolute values 0 and 0.1, there is no association, or the data is not corelated. But if it's greater than 0.5, there is a strong correlation between variables.

Understanding Feature Selection Using Filter, Embedded, and Wrapper Methods
It's quite possible that your data contains many features that are not really relevant or are insignificant. They don't contain much predictive power. This is where you would choose to use feature selection to select the most relevant features. Feature selection is applied when you have too much data to train your machine learning model, and not all of the data makes sense. Feature selection is one of the techniques that is applied to a larger problem where you try to reduce the complexity in your data. You can reduce complexity using feature selection or dimensionality reduction. Feature selection involves the application of different techniques to select the most relevant features and those are the features that you'll use to train your model. Now these techniques can be divided into three broad categories, filter methods, wrapper methods, and embedded methods. Feature selection involves choosing a subset of relevant features, it does not actually change the X variables or features that you're working with. On the other hand, you can also reduce complexity using dimensionality reduction. Dimensionality reduction involves reorienting your original features along new axes to better express your data. With dimensionality reduction, the original X variables are transformed or changed. Dimensionality reduction can be performed using projection where you project your data along new axes; manifold learning, which unrolls complex data in a higher dimensional space; or autoencoding, which is a deep learning technique to extract latent features within your data. We'll focus our attention here in this module on feature selection techniques, using filter, wrapper, and embedded methods. We discuss briefly when you would choose to apply feature selection as a part of your feature engineering. You have many X variables, most of which contain very little information, but there are a few that are meaningful, and these meaningful variables are independent of one another. That's when feature selection works best. Feature selection techniques fall into three broad categories, filter methods, embedded methods, and wrapper methods. Let's talk about filter methods in more detail first. Filter methods allow you to select features, or columns in your data, independently of the algorithm that you're going to use to train your machine learning model. It doesn't matter what model you're going to build, whether it's a decision tree, a simple linear or logistic regression, or a support vector machine model, these have no bearing on the filter methods that you use for feature selection. This is because filter methods rely on statistical properties of features in order to select those features that are the most relevant, which have the most predicted for your target or label. You can apply statistical techniques to determine the relevance of each of your X variables individually. This is referred to as univariate analysis. You may also want to apply these statistical technique s jointly to multiple columns at a time, that is multivariate analysis. And when you apply these techniques using Python libraries, it's not necessary for you to know the exact implementation of the statistics behind these techniques. All you need to understand is the intuition. You can also select features using embedded methods for feature selection. Here features or columns are selected during the actual model training process. There are certain machine learning models which assign importance or ranks to individual features. Examples of such models are decision tree models and lasso regression models. Once feature ranks or importance measures have been assigned to the individual features, you can use these metrics to select those features that are the most relevant. These are referred to as embedded methods because the feature selection process is effectively embedded with the modeling process. Feature selection is a part of model training. Embedded methods are useful, but not all machine learning models support feature importance ranking, only specific types of models perform feature selection. So if you want to use an embedded method for feature selection, you'll have to choose the right kind of machine learning model that supports this. Wrapper methods lie somewhere between filter methods and embedded methods. Filter methods perform feature selection completely independent of the model that you're going to fit. Embedded methods embed the process of feature selection within model training. With wrapper methods, features are chosen by building different candidate models. So you basically generate a number of candidate models on different subsets of your features. You find that model that performs the best and then choose the feature subset that was used to train that model. Forward and backward, stepwise regression are examples of wrapper methods. Forward stepwise regression involves adding a new feature to train each candidate model. Once a feature is added, if the model improves, that feature becomes a part of the selected features. With backwards stepwise regression you do the same thing, but at every step you remove a feature from the existing set of features. Wrapper methods of feature selection can get computationally intensive because you're basically generating many, many candidate models, each with a different subset of features. But all candidate models are similar in structure. For example, if you're fitting decision tree models, all of the candidate models will be decision trees. There are different wrapper methods based on how you select the subset of features to train your model. Features may be added or dropped to see whether the model improves, or you can recursively select subsets of different sizes and train candidate models.

Feature Selection Using Missing Value Ratio
When you're building and training machine learning models for predictions, you may have a large number of features to work with, but not all features might be relevant. Here in this demo, let's see how we can perform feature selection using the missing value ratio. Ideally, to build and train our model, we'd like to use only those features that have missing values below a threshold. The dataset that we'll use here is the Melbourne housing dataset available here at this URL. I've downloaded it as a csv file onto my local machine. I read this into a pandas data frame, and here is what the data looks like. All of the columns here are characteristics of homes in Melbourne, where they are located, the seller who has listed it, a bunch of other information, and we can use all of these features to predict the price of a home. Now this dataset is fairly large. If you take a look at the shape, you'll see that we have 13, 580 records. You'll find that some of the columns in this dataset contains missing values. Let's see which columns they are by calling isnull.sum and sorting the columns based on the number of missing fields. You can see that certain columns, such as BuildingArea and YearBuilt, have a large number of missing values and others, such as Car and CouncilArea, have fewer missing values, relatively. A quick way to see what proportion of our original dataset has missing values is to use this formula here. Calculate all of the null values and divide by the total number of records that we have. This will give us a number between 0 and 1 representing the proportion of missing values. Almost 50% of the records don't have information for BuildingArea, 40% for YearBuilt, 10% and under for Car and CouncilArea. Now let's zoom in on just these four columns here, BuildingArea, YearBuilt, CouncilArea, and Car. And you can see that there are a large number of missing values, we are likely going to have to fill in missing values and drop features which have too many missing values. I'm going to keep the missing value numbers on screen so that we know what we are dealing with, and let's invoke describe on these four columns. This will give us a quick overview of this data. BuildingArea, YearBuilt, and Car are numeric features and you get descriptive statistics for these features. You can see that the medium value for the number of cars owned by these households is equal to 2. And there are just 62 missing records in the Car column. So I'm going to go ahead and fill in these missing values using the median value of 2. And for this, the pandas fillna function works very well. Once we've filled in the missing values, let's run describe on these four features once again, and you can see that the median value for Car hasn't changed. Filling in with the median value helps keep the central tendency of your data where it was at originally. Now of the four original features that had missing values in our dataset, we've already filled in the missing fields for the Car column. We can now deal with missing values in the other columns, and I'll move on to the CouncilArea column, which has about 1300 records with missing fields. Many of these records are at the tail end of the dataset, so I'm going to invoke the tail function in order to explore these records. You can see that the CouncilArea has NaNs in here. Let's take a look at the most common CouncilArea in our dataset, which we can get by invoking the mode function on this column. The most frequently occurring CouncilArea is Moreland. So a good way to fill in missing values for CouncilArea is to say the most commonly occurring one is likely to be the CouncilArea for the missing values. Remember this will not be precise, but is a reasonable assumption to make if you have few missing values in your dataset. Once we've filled in the missing values for CouncilArea, let's take a look at the tail of our data and you can see that the original NaN columns now have Moreland. We are left with two columns now with missing values, BuildingArea and YearBuilt, and the proportion of missing values in these columns is rather large, over 40%. If you take a look at the columns in this dataset, you'll see that we have a fairly large number of features to work with. When you have a large proportion of missing values for any feature, it's rather hard to use techniques to fill in these missing values because you don't have enough data to deduce what the missing values are. It's better to drop those features entirely. And that's exactly what I've done here. I've used the dropna function to drop any feature which has more than 10% missing values. So the columns that I'm left with no longer include the YearBuilt and BuildingArea columns. And this is how we perform feature selection using missing value ratios. Let's take a look at our data now and you can see that we have no columns with missing values. This is a dataset that we can use now for machine learning. I'm going to write out this dataset to a csv file called melb_data_processed.csv. And you can see it right there under our datasets folder. This is our cleaned up data with no missing value.

Calculating and Visualizing Correlations Using Pandas
One technique that you can use to build robust machine learning models is to train that model using features that are uncorrelated. Uncorrelated features lead to more robust models. Here in this demo, we'll explore a few different techniques to calculate the correlations that exist between features. Because we're going to focus a lot on visualizing these correlations, I've also imported the seaborn library here. The seaborn library is built on top of matplotlib and gives us some cool visualizations that we'll see in just a bit. If you don't have seaborn installed, you can get it with a simple pip install seaborn. We'll work with the same Melbourne housing dataset as before. We'll work with the processed file, which contains no missing values. You're familiar with the data itself, let's convince ourselves that this indeed contains no missing values, isnull.sum shows as all 0s. Let's explore the statistical characteristics of this data, invoke the describe function on my data frame, and I transpose it so that it's easy to view and round off all of the statistics to two decimal places. After filtering based on the missing value ratio, here are the numeric features that we are left with. In order to keep things simple, I'm only going to calculate the visualize the correlations that exist between my numeric features. As you'll see in just a bit, there are different kinds of correlation values that we can calculate. Some work with categorical data, some don't. To keep things simple overall, we'll only work with integers and floats, that is numeric features. I'll go ahead and select all of the numeric columns from our Melbourne housing dataset. So select only those columns where the data types match the numeric data types that I've instantiated in the list earlier. This leaves us with a number of interesting features to explore, the rooms in a house, the price of a house, the distance from the city center, bedrooms, the number of cars in a family, and so on. I'm going to extract a data frame with just these numeric columns and store it in the clean_data_numeric df. If you take a look at a sample of this data frame, you'll see the 11 numeric features that we have to work with and the shape of the data shows us that we have over 13, 000 records and 11 columns. In order to further trim the number of features that we are looking at, I'm going to drop three features which I consider less interesting, latitude, longitude, and the postal code. Once we get rid of these columns, we have eight features that we can use to calculate correlations and they are all non-null with no missing values. Correlations are the measure of a relationship that exists between two variables. If you want to view the correlation coefficients for every pair of variables that exist in your data frame, you can invoke the core function on your pandas data frame that gives you a correlation matrix. Correlation coefficients can be calculated using different statistical techniques and the method that we've specified here is the Pearson's correlation. This measures the strength of the association between any two variables, and also specifies the direction of the relationship. This gives us a correlation coefficient between -1 and 1. A negative sign indicates that the two variables move in opposite directions, whereas a positive sign indicates that the two variables move in the same direction, when one increases, the other increases as well. A value of 0 indicates no correlation at all. Pearson's correlation coefficient that we calculated here is a measure of the linear relationship, or the linear correlation between two variables, and this is by far the most widely used correlation statistic. And here is our correlation matrix. Observe that every variable is perfectly positively corelated with itself. It has a correlation coefficient of 1. You can see that the number of rooms in the house is positively corelated with the price of a house, which intuitively makes sense. A larger home is more expensive. The number of bathrooms in a home is also positively corelated with the number of rooms. Greater the number of rooms, greater the number of bathrooms available for the individuals living there. Observe that the price of a home is negatively corelated with its distance from the city center. Homes that tend to be further away from the city tend to be less expensive. All of this makes sense. Here is a visualization of the same correlation matrix using a heatmap, which uses colored cells to display the strength of a relationship. The default color displays positive correlations using a light creamish color and negative correlations using a darkish black color. Another statistical technique that you can use to calculate correlations is Spearman's correlation. This applies to sorted variables as well. That is categorical variables that have some kind of ordering. The earlier technique, Pearson's correlation, is not used with categorical data unless you have just binary categories. For example, a star rating, 2, 3, 4, has an intrinsic order and it's a categorical variable. Spearman's correlation measures the monotonic associations between two variables. That is, it measures strictly increasing or decreasing relationships. Here is the correlation matrix with Spearman's correlation. You can see that the numbers are a little different. And here is the same correlation matrix visualized in the form of a heatmap. Pandas allows you to calculate correlations using yet another technique, that is Kendall's correlation. Kendall's correlation is often used with discrete categorical data, like the star rating we spoke of earlier. However, it does not take into account the difference in ranks. It does not see that a three star is very different from a one star, for example. It takes into account that a three star is better than a one star, but not how much better. And here is the Kendall correlation matrix, that we just calculated, visualized in the form of a heatmap.

Calculating and Visualizing Correlations Using Yellowbrick
A different library that you can use to calculate and visualize feature correlations is the yellowbrick library. This is an open source Python project which extends the scikit-learn API with visual analysis and diagnostic tools. This yellowbrick library offers you tools that are primarily focused in helping you select the best features of your model and the best model overall. Let's take a look at how we can use yellowbrick to calculate and visualize feature correlations. Yellowbrick allows us to calculate correlations versus a certain target variable. The target variable that I've selected is the Price column, and the features are all other columns except Price. We'll use yellowbrick to see how all of the features are corelated with the price variable. Go ahead and import the feature correlation object from the yellowbrick target namespace and let's set up a list of feature names. We are now ready to instantiate the feature correlation, visualizer object. We pass in the feature names, they make up our labels, and the correlation method that we use here is the Pearson correlation. Call visualizer.fit on the features and the target of the correlation calculation. Visualizer.poof will display a visualization showing you the correlations that exist in your data. And yellowbrick gives you this very nice visualization of the correlations that exist in your data using horizontal bar graphs. There is a zero line at the center here. You can see that distance is negatively corelated with price, we already knew that, and most of the other features are positively corelated. The length of this bar graph gives you the strength of the correlation. The feature correlation object supports other techniques to calculate correlation as well, such as the mutual information. This estimates the mutual information for a continuous target variable, mutual_info-regression. Our target variable here is price, which is a continuous value, which is why we use mutual_info-regression here. If you have discrete features amongst your feature names, you need to explicitly specify those because mutual information calculation is different for continuous features versus discrete features. Mutual information is a measure that tells you how much information you can gather about your target, that is the price, by observing another random variable, that is the individual features. Let's take a look by calling. Mutual information specifically measures the amount of information obtained about one random variable through observing another random variable. And here is a visualization of correlations using mutual_info-regression. Mutual information contains values from 0 to 1. So it does not specify the directionality of the relationship.

Feature Selection Using Filter Methods
In this demo, we'll see how we can select features that are relevant to the kind of model that we want to build using different feature selection techniques. We'll start this demo off in a brand new Jupyter Notebook called PerformingFeatureSelection, and the dataset that we'll work with is the Boston housing dataset. This dataset is one of the most popular ones used for regression analysis. We'll use the characteristics of a home in order to predict the price of a home in Boston. We'll use a number of feature selection techniques to select the most relevant features and compare the different models that we've built. This dataset is fairly small, just around 500 records. Here is a sample of the dataset that we are working with. You can see that the column names are rather cryptic. I'm going to rename the columns of this dataset so that the names that we use for the columns are meaningful and indicate what the column is about. Crime rate, the zoned ratio within a city, the industrialization ratio, whether the house is along the Charles River or not. These are some of the features that we'll use to train our regression model. Go ahead and assign these new names to the columns of our housing data and let's take a look at a sample of our data now with the new column values. They are now easier to read, we know what a column stands for. I should tell you though that the Boston housing data that I'm using in this demo is a slightly modified form of the original. I've trimmed out some of the columns that I felt were less relevant. Missing values in this dataset are represented using the string NA, so I'm going to replace all NA strings with np.nan, and I'm then going to invoke the isnull.sum function to see how many records have missing values. There are about 20 records in our dataset with a few of the columns containing missing values. I'll just go with a simple technique here and fill in all of the missing values with the average or the mean value from each column. Once the missing values have been filled in, let's take a look at our data, it should contain no missing values. We are now ready to work with this data. Because we chose to fill in missing values rather than dropping records with missing values, we still have 506 records to work with. The target, that is the target of our prediction, will be the median home value, the price of a home in a town. And the features that we'll use to train our regression model is all of the other columns, except the MedianHomeValue. Drop the MedianHomeValue, the rest are our features. Before we go ahead and apply statistical techniques to find which features are the most relevant for our regression model, let's visualize the feature correlations that exist in our data. We'll have all of our feature names as labels and we'll fit our features versus the target, and here is our Pearson's correlation visualized using yellowbrick. You can see that this dataset is interesting, there are a large number of features that are negatively corelated with the price. For example, a town with fewer teachers, that is a large pupil teacher ratio, has lower house values. A town with a high property tax rate also tends to have lower house values. There are features here that are positively corelated as well. If a house is along the Charles River, it tends to be more expensive. If a home is larger with more rooms, that house tends to be expensive as well. With this, we are ready to move on and apply our first feature selection technique. Feature selection using univariate analysis. And for this, we'll use an estimator available in scikit-learn called SelectKBest. That will allow us to select the KBest features. The SelectKBest estimator allows you to specify the statistical technique that you want to use to select the k most relevant features. The technique that we'll use here is f_regression. I've chosen this technique because price prediction is regression analysis. And this feature selection technique is a univariate linear regression test, which calculates the correlation between every regressor, that is every feature, and the target, that is the median value of a home. The SelectKBest estimator makes it very easy to select the KBest features. Instantiate the estimator object, specify the statistical technique that you want to use to select the KBest features, here we use f_regression and specify how many relevant features you want. Here k I've specified to be equal to 5. Once the estimator has been instantiated, go ahead and call fit under features and the corresponding target. Let's take a look. So which are the features that the regression analysis that we just performed considers the most relevant. You'll get a mask of the features selected by invoking get_support under selected features. A false value indicates that the feature was not selected, and a true value indicates that the feature was indeed selected. These correspond to the order in which the features were specified in our training data. We can use this feature mask on the columns of our data frame in order to get the selected features. Based on regression analysis, these are the five most significant or relevant features for regression. We can take a look at the corresponding regression analysis scores, and here they are on your screen. Higher scores here indicate more relevant features, you can see that the features that are selected are the ones with the highest five scores. These scores are a statistical property known as the F value, calculated between the features and the corresponding label. I'm going to set up a little data frame here to see what feature was associated with what score, and I'll sort all features in a descending value of Score, so that I get the features with the highest scores first. Here are the five most relevant features based on our F regression analysis. I'm now going to set up a new data frame called uni_df with the feature names that were considered to be the most significant or relevant based on our univariate analysis.

Feature Selection Using Wrapper Methods
We've got the five most relevant features using univariate analysis. In this clip, we'll use a few other techniques in order to find the most relevant or significant features. The techniques that we'll implement in this demo are wrapper methods for feature selection. The first technique that we'll study here is feature selection using RFE or recursive feature elimination. This technique selects features by recursively considering smaller subsets of the original set. At every step of this algorithm, it prunes the least important feature. Recursive feature elimination is available as an estimator object in scikit-learn and we import it from sklearn.feature_selection. While performing recursive feature elimination, we use a machine learning model that is repeatedly trained on different subsets of our features. And we find that subset of features that gets the best possible model. The machine learning model that we're using here is the linear regression model. Go ahead and instantiate the RFE estimator object and one of the input arguments to this estimator is the linear regression estimator that we'll use to repeatedly train our model on different subsets of our features. Using recursive feature elimination, we want to select the five most relevant or significant features, and at every step of this iterative process we want to prune one least important feature. This is what we specify when we say step equal to 1. Once this high-level estimator has been set up for recursive feature elimination, call fit on your data, that is the features and the target, and wait for the estimator to train the different regression models to find the most significant features. Once a subset of features have been found using RFE, we can access what those features are by accessing the support property on our RFE estimator. The support property acts as a mask when the original features that we've passed in and here are the five selected features using recursive feature elimination. Recursive feature elimination gives you a comprehensive ranking of all of the features in our original data, available using the ranking property on our RFE estimator. Here are the ranks for all of the features that we use to train our model. All of the features, which have a rank of 1, are the features that have been selected using RFE. The other ranks for these features indicate their reducing importance, they indicate what feature was dropped at every iteration of this algorithm. Property task rate was dropped first, then zoned ratio, and so on. I'll now set up another data frame called rfe_df, which holds all of the features selected using recursive feature elimination. We'll now move on and explore forward selection and backward selection techniques to select features, and for this we'll need to install an additional Python library, mlxtend, or machine learning extensions. Machine learning extensions is an open source Python library and it contains a number of useful tools for data science. The sequential feature selector in the mlxtend uses the joblib Python module in order to run multiple processes in parallel behind the scenes. Make sure that you have an updated version of joblib, and if you don't, get it with a simple pip install joblib upgrade. Once you have an updated version of joblib, you're ready to get started. Go ahead and import the sequential feature selector from mlxtend.feature_selection. This is the same object that we'll use to perform forward selection, as well as backward selection of features. We'll use this object to add or remove one feature at a time based on the performance of the regression model. This addition or removal of a single feature is iteratively done until we get to the specified number of features. Let's go ahead and instantiate the sequential feature selector and we'll use the linear regression model in order to eliminate or select features. Just like in recursive feature elimination, this wrapper method involves the training of multiple models to find the right subset of features. Once again, we'll choose to select the five most relevant features, and forward is equal to false indicates that we'll perform backward selection, we'll start off with all of the features and eliminate one feature at a time. The sequential feature selector also requires a scoring technique in order to evaluate your different models. The scoring that we'll use is the negative of the mean squared error. Mean_squared_error is a good way to compute the loss for your regression model. A high value for negative mean squared error means a low value for mean squared error, and that indicates a better model. Use the selector object to invoke fit on our features and target in order to get the five most relevant features, captured using backward selection. The k_feature_names property on the filtered features will give us the names of the features selected, and here they are, features selected using backward selection. I'm going to set these feature sup in a new data frame called back_df. These are the selected features using the backward selection procedure. Let's now use the SequentialFeatureSelector object once again, but this time we'll perform forward selection as specified by forward is equal to true. Once again, we'll use the same scoring technique, negative mean squared error, and the number of features that we want to select by adding a single feature at a time is equal to five features. We can now find the filtered features by calling fit on our data and we'll access the features that were selected using the k_feature_names property, as we did before. And here are the features selected using forward selection, and I'll set this up in a data frame as well called forw_df. The forward method gave us these five features as being the most significant.

Feature Selection Using Embedded Methods
In this clip, we'll see an example of how we can use embedded methods for feature selection. The Lasso model for regression analysis is a regularized regression model. This regularized regression model adds a penalty term to the mean squared error loss that is then minimized. This penalty term forces the coefficients of irrelevant features to be close to 0. And in this way it'll extract the most significant or relevant features. Instantiate the Lasso regression object with an alpha of 1.0. The alpha property here determines the strength of the regularization and a value of 1 indicates that strongest regularization, and call lasso.fit, fitting a regression model on your features and the corresponding target. Once you've performed regression analysis using the lasso technique, the coefficients of the Lasso model will give you an idea of the most significant features in our regression analysis. So was access the lasso coefficients and sort by coefficients in the descending order. Observe that some of the coefficients are positive, indicating a positive relationship, and others are negative, indicating a negative relationship. Those features which have coefficients equal to or close to 0 are considered to be irrelevant features. Which means we should not be sorting by coefficients, we should be sorting by the absolute values of the coefficient. So calculate the absolute values of the coefficient and display the coefficients in descending order. This will give us the most relevant features. These are the features that the most relevant, which have the most impact on the target. Using the absolute values of the lasso coefficients, we can now select the five most significant or relevant features by invoking the head function and selecting the five top values. And I'll set up these five top values in a data frame, and here are my feature names. The most relevant features selected using the lasso embedded technique. Alright, we are finally approaching the denouement of this demo. We have a number of significant features selected using a variety of different techniques, univariate analysis, wrapper methods, and embedded methods. I'm now going to merge all of these data frames together to get a single data frame with features selected using multiple techniques, I used pd.concat for this, and here is what the final_df looks like. Now that we have all of the significant features selected, we are ready to perform regression analysis and see which feature selection works best for our use case. In order to be able to build and train different regression models and print out their testing scores, I'm going to use a little helper method here called best_score. Best_score takes in three input arguments, the name of the technique used to select features, the x variables or the features that we'll use to train this model, and the y values, or the target that we'll use to train our model. We'll use scikit-learn's train_test_split function in order to split our data into training data and a test subset. Instantiate a linear regression estimator object and call fit on the training data. Once you have a trained model, print out the name of the technique that we used to perform feature selection and print out the training score, that is the r squared score on the training data. We'll then use this model for prediction and print out an r squared score on the test data as well. With this helper method set up, let's build and train regression models using features selected, applying different feature selection techniques. We'll first find the best score using features selected using univariate analysis. These are the features that we got using SelectKBest and the F regression analysis. The training score is 0.63 and the r squared score on the test data is not that great, only about 0.51. The next model that we'll build and train will use features selected using the recursive feature elimination method. And this model gets a training score of 0.58, but observe that the r squared score on the test data is even higher, 0.79. This is clearly a robust model. The next model that we'll build and train will use features selected using the backward selection technique. This has a training score of 0.61. The r squared score on the test data is a little better, it's 0.64. Still not as good as the recursive feature elimination technique. We'll then move on the building and training a model using features selected using forward selection. Let's see what the r squared score is on the training and test data, and you can see that the scores are similar to backward selection, but not as good as the features that we got using recursive feature elimination. And the last model that we'll evaluate here will be trained on features selected using an embedded method, that is lasso regression. The training score here is 0.6, not bad, but the r squared score on the test data is much lower, 0.52. So overall we've seen that for our data recursive feature elimination performed the best by far.

Module Summary
And with this demo, we come of the very end of this module on understanding and implementing feature selection. We started this module off by understanding when we would choose to use feature selection and the different techniques that are involved. We'll then discuss each of these techniques in detail and also apply them in a hands on manner. We discussed filter methods for feature selection where you use statistical techniques to select those features that are the most relevant. Filter methods for feature selection are completely independent of the model that you choose to build. We then moved on to discussing and implementing embedded methods for feature selection. We saw that these are so called because the feature selection technique is embedded within the model building. The actual process of training an ML model assigns importance measures to features, and we can then select those features that are the most important. We also discussed and implemented wrapper methods for feature selection. This involved training many different candidate models on different subsets of features and the model which performed the best, we chose that feature subset for our final model. And along the way, we also discussed the different types of data that you would work with in ML models, numeric data and categorical data, and their subcategories as well. We also discussed three measures of correlation, Pearson's, Kendall's, and Spearman's correlation to work with these different kinds of data. In the next module, we'll explore feature extraction techniques, specifically how we'll extract features from image and text data.

Exploring Feature Extraction Techniques
Module Overview
Hi, and welcome to this module on exploring feature extraction techniques. Feature extraction is an integral part of feature engineering. Machine learning models can work with data that are not originally represented in the numeric form. But before you can feed such data into your ML model, it has to be encoded in a numeric format somehow. And a part of this preprocessing is the extraction of features. In this module, we'll see how you can represent images as numeric data in the form of matrices. We'll then see how we can apply different algorithms to extract features from images and represent features in the form of key points and image descriptors. ML models can't work with text directly. We'll study how you can represent text as numeric data and then extract features from text. In this context, we'll talk about TF-IDF representations of text data and prediction-based embeddings. In an earlier module, we discussed all of the components that make up feature engineering and we discussed that feature engineering was somewhere between art and science. Feature extraction defers from feature selection that we discussed earlier. This is because we are not selecting a subset of input features, we are deriving new features from an original set, and these derived features may not be recognizable.

Representing Images as Matrices and Image Preprocessing Techniques
Machine learning models can only understand data which is expressed in numeric form, which means when you work with image data, or even text data, you need to convert these to a numeric format first. When you work with image or text data, there are a number of preprocessing steps involved before you can use your data to feed into ML models. All of this is a part of feature extraction and these steps tend to be time consuming. These steps are a major bottleneck to scaling. Before we get to feature extraction from images, let's talk about how we can represent image data in numeric form. Now all images are essentially a grid of pixels. If you have color images, which are RGB images, every pixel in the image is represented using three values, R, G, and B. And R, G, and B values are typically integers in the range 0 to 255. So a pixel that is red in color will have RGB values of 255, 0, 0. A green pixel will have RGB value of 0, 255, 0. And a blue pixel will have RGB value 0, 0, 255. Color images have three channels and they're also referred to as multichannel images. Three values to represent colors, three channels in the image. Now on the other hand, you might have grayscale images. Pixel values in grayscale images represent just intensity. So they are typically values between 0 and 1. Each pixel is represented using one value, one value to represent intensity, one channel in the image. This is a single channel image. So each of these images are essentially matrices. They are three-dimensional matrices, height, width, and number of channels. Now in the world of machine learning, multidimensional arrays or matrices are often referred to as tensors. So images are represented using 3D tensors. A tensor is simply a matrix that can run on a graphical processing unit. Observe that 6, 6 refers to the height and width of both of these images. The last dimension here refers to the number of channels. When you're training your machine learning models, whether it's a neural network model or a regular one, images are generally fed in batches. So you have a list of images in a batch that is represented as a 4-D tensor or a four-dimensional matrix. Now in order to use a single 4-D tensor to represent a list of images, this requires that all of the images should be the same size. Let's take a look at the dimensions of a tensor or a matrix, which is a list of 10 images. Here is what it would look like. Here are the dimensions. The very last dimension here refers to the number of channels. This list here is a list of multichannel color images. The two dimensions here at the center refer to the height and width of the individual images in the list. Each of the images are 6 x 6 images here. And finally, the very first dimension refers to the number of images. Here we have 10 images in the list or in this batch. Once you've represented your images in the form of a matrix, you'll then apply a number of image pre-processing techniques before you feed in your images to your ML model. Image pre-processing techniques can be thought of as a part of image processing and feature extraction. These help build more robust models. It's helpful if you have all of your images have a uniform aspect ratio. The aspect ratio is the ratio of the width to the height of individual images. Another technique is to have a uniform image size. All of the images that your model sees should be of the same size. You can apply resizing and rescaling techniques so you get images of the same size. You may also want to make your models more robust by feeding in images which are perturbed. You might disturb the image in some way by adding some kind of transformation or perturbation to the images and see whether your model can still work with those images. Deep learning framework, such as neural networks, work better when the data that it has to work with is centered around a mean of 0. So another common image pre-processing technique is to normalize your input images so all images have a mean of 0 and their pixel values are within a certain range. Now image data can get very, very large, it's of very high dimensionality, and not all of the pixels in your image are actually useful. When you work with images, it's very common to apply dimensionality reduction techniques to reduce the complexity of your data. And finally, another common preprocessing technique used with images is data augmentation where you augment or change your image in a certain way to build more robust models.

Feature Detection and Extraction from Images
You have machine learning models that work with images directly after applying a number or pre-processing techniques to the input image matrices. You can also apply algorithms to perform feature detection and extraction from image data. Feature detection and extraction in the context of image processing refer to those algorithms that detect and extract the appropriate, most interesting features from images. And these feature representations can then serve as an input to other image processing models or techniques. Working with images, using machine learning models is a very hot topic nowadays. Working with image data is a hot topic of research for data scientists and there have been many breakthroughs recently. When you're working with images, an important first step is feature detection. This involves finding the right features and the right feature representations of images, and this, in fact, is the starting point of many computer vision algorithms. The features that you detect serve as an input to other algorithms. In feature detection, repeatability is an important factor. Something that you identify as a feature in an image once, you should be able to identify as the same feature again. Once you've detected the right features, this is often used in feature matching algorithms to see whether two images are of the same object. Finding the right features and representing them in the right manner is an extremely important component of feature matching. And this is where repeatability will determine whether the same feature will be detected in two or more images. So what are the kinds of features that you might want to detect within an image? Features can include abstractions, such as the style or texture of images. This includes color. Features can also include the actual content or structure of an image. You have points of interest, you have edges, you have corners. You may also have regions of interest or blobs. All of these features should be stable and repeatably identifiable. So here are the basic components of feature detection. Which points within your image are the most interesting? These are referred to as interest points or key points. Which regions or blobs within your image are the most interesting? And finally, what is interesting about all of these? Figuring out which points are the most interesting is referred to as the detection of interest points. Interest points are also referred to as key points. Figuring out which regions are interesting involve the detection of blobs or areas of interest. And finally, how would you represent what is interesting about the points of interest, as well as area of interest? Well, you need to compute image descriptors. Let's first talk about what key points are within an image. Points in the image that define what is interesting and must be captured in the feature representation of an image. Extracting key points is an important part of extracting features from images. Key points or interest points should be well defined and clearly identifiable. For example, corners might include an interest point, these are the intersection of two edges. If you have intensity maxima or a minima within an image, that is an interest point. Line endings are key points. Key points should be stable and they should not be affected by common transformations, such as rotation, translation, expansion, or warping. Blobs or regions of interest within an image are interesting regions within which points are similar and share properties that are different from surrounding points. If you have a portion of the image that has the same intensity or color, that is a blob. Blob detection applications are complementary to points of interest and they capture additional information about an image, and they are very commonly used in object recognition, motion tracking, texture analysis, and image segmentation. And once we know the points and areas of interest, we need to be able to describe them using image descriptors. Image descriptors are just numeric arrays which give you descriptions of key features of images, such as shape, color, texture, and even motion. Image descriptors can be computed using a number of different algorithms. Good image descriptors should be independent of the position of the associated key points. Good image descriptors should also be robust to transformations, such as warping or scaling of images. They should also be scale independent. The same descriptor should be able to describe the key point at any scale of the image. Image descriptors are extremely useful because they're used for feature matching or pattern matching across images. Descriptors are just vectors of numbers or arrays, and they help compare key points across two or more images. You can then perform some kind of distance computation to see whether the patterns in the two images, or key points in the two images, are close to one another. Key points whose descriptors have the smallest distances are matches across the two images. There are two popular algorithms for feature extraction using image descriptors and key points. SIFT, or the Scale Invariant Feature Transform, and DAISY descriptors.

Feature Extraction from Text
Now we know that machine learning models can work with text data. You know the variety of natural language processing applications that are out there. But if you try to feed your ML model raw text, that won't work. You need to extract features from text, you need to encode your text in numeric form. In order to be able to represent your text in the form of numbers, you need to model your document as a word sequence. Any document that you're working with is simply an ordered sequence of words, you can then tokenize your document and extract the individual words from that document. Then in order to convert this document to be represented numerically, you can represent each word as a number. So you've tokenized the words, now you find some kind of number to represent this word, and this number is a numeric encoding of that word. This means there will be a unique number associated with every word in your document, and your original document can now be represented as a sequence of numbers. So your text document is now a matrix or a tensor of numbers. Now the big question that arises where is how best can words be represented as numeric data? Now this is a fairly important question. We don't want just random numbers to represent individual words. It would be great if somehow the number used to represent a word captures meaning and maybe semantic relationship that the word has with other words in the context. Now there are several techniques out there which you can use to represent text in numeric form. Think of the numeric encoding of a word as a word embedding. You have one-hot representation, you have frequency-based representations of words, and then you have prediction-based representations. All of these are feature vector representations or words and the simplest of these is one-hot encoding. With one-hot encoding, you construct a feature vector which basically represents all of the words in your vocabulary and the presence or absence of a single word in your document is indicated by a 1 or a 0. One-hot encoding only indicates the presence or absence of a word. It does not capture any semantic information or relationship between words. Then you have frequency-based encodings, which is a step up from one-hot encoding. Frequency-based encodings try to capture how often a word occurs in a document and also across the entire corpus. When converting text to numeric form, there are three common techniques you can use for frequency-based encodings, count vector encodings, TF-IDF, which stands for term frequency inverse document frequency, and, finally, co-occurrence matrices. When you use count vector encodings to represent text, you'll capture how often a word occurs in a document, that is you'll capture the counts or the frequency of a particular word within a document. This is a step up from one-hot encoding, which only captures the presence or absence of a word. A feature extraction technique, which can be better than count vector encodings, is the use of TF-IDF scores. TF-IDF stands for term frequency inverse document frequency, and this captures how often a word occurs in a document, as well as across the entire corpus. The TF-IDF score for a particular word is a combination of two scores. The term frequency, if a word occurs more frequently in a single document, that word might be important. Higher term frequency scores indicate that a word occurs frequently in a single document. The second component of TF-IDF is the IDF score, which stands for inverse document frequency. Now if there are words that occur frequently in the corpus, it's probably a common word, a stop word such as a, an, the, this, etc. Common words have lower IDF scores. And finally, the last prediction-based technique for word encodings that we'll discuss here is the co-occurrence matrix. This matrix tries to capture a word and other words that occur in the context of that word. And this is based on the principle that similar words will occur together and will have similar context. Of all of the techniques that we've discussed here, none of these, none of the techniques that we've discussed so far capture the meaning or the semantic relationship that a word has with other words. And this is where prediction-based encodings come in. Predication-based word embeddings or word encodings are numerical representations of text which capture meanings and semantic relationships. And of all of the techniques that we've discussed so far, this is by far the most powerful, but it's also the most complicated to generate. Generating prediction-based word embeddings involves the use of a machine learning algorithm. You have an ML-based algorithm, which has been trained on a large corpus of text data, typically all of Wikipedia. You'll feed in the word London, and it'll generate a numeric representation of this word, and this numeric representation will capture meaning. For example, the embedding for the word London might be very close to the embeddings generated for words like Paris or New York, all of which are large cities like London. The corpus that your machine learning model needs to train on has to be a very large corpus, all of Wikipedia, for example. This ML-based algorithm that generates word embeddings is typically an unsupervised learning algorithm, an algorithm trained on data with no labels. You can use neural networks, deep learning, nearest neighbors. There has been a lot of research on this topic and there are pretrained word vector embeddings available for you to use out there in the real world. You don't have to generate these yourself. In fact, it would be fairly hard for you to generate this yourself. You would need a lot of training data and a lot of resources for distributed training.

Module Summary
And with this, we come to the very end of this module on exploring feature extraction techniques. In this module, we focused our attention on image and text data. We saw how we could represent images in numeric form, in the form of multidimensional arrays or tensors, and we also discussed how we could extract features from images and represent the features that we detected and extracted in the form of key points and image descriptors. We then moved on to discussing text data. Text data has to be encoded in numeric form before we can use them in training our model. We discussed the techniques that we could use to convert text to numeric form. We also discussed extracting features from text data. Machine learning models work with numeric data. Other important kinds of data that we need to be able to work with is text and image data. That's why we've focused our attention on those. However, feature extraction is possible for all kinds of data. You have geospatial data, you have time series data, you have text in images for which you'll use OCR to extract features, that is optical character recognition, and you can also extract features from date and time data. All of these we'll look at in the next module in a hands-on manner.

Implementing Feature Extraction
Module Overview
Hi, and welcome to this module on implementing feature extraction using a variety of different techniques on different kinds of data. Now in the last module, we had a conceptual overview of feature extraction from text and image data. In this module, we'll get hands on. We'll see how we can implement feature extraction from text data. We'll perform tokenization of text, lemmatization, and vectorization. We'll then perform image transformations on images and perform feature extraction using the SIFT algorithm. We'll also see how these SIFT descriptors can be used in feature matching algorithms. We'll also see how we can extract text data from images using optical character recognition, or OCR. For this we'll use Google's Py-tesseract library, a Python wrapper around the original tesseract engine. We'll also work with data that has date information and we'll see how we can extract features such as month, day, and year from the date information. And finally, we'll work with geospatial data using geopandas. We'll see how we can use points, lines, and polygons to represent geographic features.

Tokenization and Visualizing Frequency Distributions
In this demo, we'll see how we can extract features from text data. We'll first normalize our input documents by using techniques such as lowercasing and lemmatization. We'll tokenize our documents into individual words and then apply vectorization techniques to extract features from text. If you're working with text data for natural language processing, in addition to scikit-learn, the nltk, or the natural language toolkit for Python, is an extremely useful library. You can use a pip install in order to get nltk onto your local machine. In order to perform normalization of our text, we'll use an additional library here called inflect. Inflect allows you to correctly generate plurals, singular nouns, ordinals, or indefinite articles. It can also be used to convert numbers to words. Set up the import statements for the libraries that you will use, in addition to pandas and matplotlib, we'll also use the natural language toolkit, specifically the word tokenize function, allowing us to tokenize documents into words. The natural language toolkit works on a corpus of text data from which it has learned information. There are several corpuses associated with nltk, wordnet is one which is used for lemmatization of your text. In order to use the wordnet lemmatizer, call nltk.download wordnet. In addition, we'll also download the punkt corpus, which is needed for word tokenization called nltk.download punkt. Once we are set up, let's extract a few documents to work with using this dataset here. This is the Amazon babe reviews dataset available at this Kaggle link here. We'll not use all of the reviews, we'll just select the first few. Observe that it has a bunch of information about baby products. The review text column is what we're interested in. This happens to be a rather large dataset with almost 60, 000 records. We won't be using anywhere close to all of this data. We'll just extract the reviewText. You can see here the reviewText in the data data frame and after reviewText, we'll work with just the first six records. This is so that we can see how the different operations affect our data. In order to keep things simple, I'm going to convert this dataset with six documents into a list of documents that I stored in the documents variable. Let's take a look at the first review here and you can see it's a tracking book for all baby related activities, feeding, waking up, and so on. Let's tokenize all of our documents into individual words and we'll store these word tokens in the document word tokens list. Iterate through all of the documents that you have and invoke the word tokenize function on the document. The list of word tokens generated from each review I append to the document word tokens list. Let's take a look at one review here. The review at index 0, completely tokenized using the word tokenizer from nltk. You can see that this is a list of individual words from the review, including punctuation. Let's take a look at the work tokens from another document, the one at index 2, and here are the individual word tokens. Another baby product review. There is a frequency distribution object available in nltk that allows you to view the frequency distribution of words within your text. You can initialize this frequency distribution object using the words for a single review or across your entire corpus. Once you have the frequency distribution object, you can invoke the most common function in order to view the most common words. Here we'll see the top 15 words from that single review. The review was for a baby book and the word book appeared twice. You can use matplotlib to visualize this frequency distribution as well, called fdist.plot, and say cumulative equal to False, and this will give you a nice line plot showing you the frequencies of the different words in this review.

Performing Normalization Using Different Techniques
Now that you know how the word tokenizer works, let's talk about the different normalization operations that you can apply to your review. These are pre-processing techniques that you'd use before you build feature vectors from text data. These pre-processing techniques serve to get rid of unimportant or irrelevant features from your text. Set up the imports as you see here on screen, observe that we are importing a stopwords corpus, as well as a lemmatizer that we'll use to convert our words to root form. When you're working with text data, maybe to perform classification or sentiment analysis, the case, whether your data is in uppercase or lowercase, may not really matter much. So a normalization technique that is often used is to lowercase your text documents. Here I run a little for loop to iterate through all six documents and convert them all to lowercase and then return. Another normalization technique used in natural language processing is to remove punctuation because punctuation may not contain much information. Here I iterate through all of the documents once again and I use a regular expression to get rid of periods and other punctuation marks. This regular expression replaces punctuation with the empty string. and every non-empty string, we append to the original documents list. Another example of a normalization technique that you might use on your text data is to replace numbers or digits with their equivalent words, and we'll perform this operation using the inflect engine. This is an object in the Python library that has support for numbers to word conversion. In order to check whether a particular word is a digit, we'll need to split each review, that is each document, into their corresponding word tokens. And we iterate over each word and run a little if check to see whether it's a digit. If it is, we'll use the inflect engine to convert this digit to the corresponding word, and if it isn't, we'll directly just append the word. We'll recreate the original document from its words by simply joining all of the words together using the space character. Another common normalization technique that you can use with text data is to lemmatize your words. Lemmatization involves using a corpus to reduce individual words in your document to its root foot, known as the lemma. The corpus is needed because the root form of the lemma of any word is an actual word in the dictionary. Let's lemmatize our verbs here using the WordNetLemmatizer. This is a lemmatizer available as a part of the natural language toolkit in Python. You can tokenize your documents using word tokenize or simply call document.split, iterate over individual words, apply the lemmatizer.lemmatize operation for all of the verbs, and find the root form for each verb in your text. Recreate the document from the individual words by joining on this space. This is lemmatization that we just performed. I am now going to put all of the helper methods that we've set up so far into a single function called normalization. It'll convert o lowercase, remove punctuation, replace numbers, and lemmatize verbs. Alright, here is the original document at index 2. We'll now apply a number of normalization techniques and take a look at the result. The first normalization technique that we'll use is the to_lowercase, which will convert all of our text to lowercase. You can see that each capitalized word is now present in the result in its lowercase form. Let's move on to another normalization technique. We'll now remove punctuation on our lowercase documents. We used a simple regular expression to remove punctuation and it got rid of all of the periods that were present in the original text. Let's move on and replace all of the numbers with their equivalent words. You can see that the numbers 12 and 2, which are individual words in the original document, have been converted to their word equivalents. This particular document that we are looking at, the one at index 2, doesn't really have many verbs. So in order to see how lemmatization works, I'll use some other text here. This is just some text that I picked from a random Wikipedia article called Lemmatize verbs on this text, and you can see what the result looks like. Observe that all of the verbs have been converted to their root form because that's what we specified in our lemmatization. Are is converted to be. Travel and traveling are both converted to the root form travel, and lie and lying are converted to lie. Let's now run lemmatization on our Amazon reviews and take a look at the document at index 2. You can see that there are not really many verbs here, which is why there wasn't much of a change. And of course, if you want to apply all of these normalization techniques in one, go to your input documents, you can simply invoke the normalization function. And here is our document at index 2 once all of the normalization techniques have been applied. Machine learning models

Creating Feature Vectors from Text Data
can only work with text in numeric form. In this clip, we'll see how we can create feature vectors from our text data, numeric feature vectors that we can use to feed in machine learning models. There are different techniques that you can use to extract features from text data. The count vectorizer in scikit-learn can be used to represent words in a document using the frequency of individual words. Instantiate the count vectorizer estimator object and call fit on the six documents that we've been working with. Under the hood, the count vectorizer will tokenize the individual review text and then extract the vocabulary. The vocabulary is made of up all of the unique words across your entire document corpus. You can see that the count vectorizer has automatically lowercased all of the words in your review text and associated unique numeric identifiers or integers with each word. These integers will represent the position of that particular word in each feature vector that we use to represent a document. We've been working with six documents, that is six reviews, and the total length of our vocabulary is 367, which means that each feature vector generated using the count vectorizer will be of length 367. In order to represent our documents using feature vectors, we'll use this vectorizer to call fit_transform on our documents. Observe that the resulting feature vectors have the shape 6, 367. There are 6 documents in our corpus and the number of unique words in our vocabulary is equal to 367. The length of the feature vector used to represent each document is equal to the size of our vocabulary. When you use the count vectorizer, our feature vector in the doc_terms variable is represented using the frequencies or the number of occurrences for each word within your document. The document that is represented here is the first document at index 0, the word index is the unique numeric identifier for each word and the number of occurrences correspond to the frequency of the words. The default here is the sparse matrix representation, you can convert these to dense feature vectors by invoking the toarray function. I'm going to go toarray on the first document, and here is a feature vector representation using the count vectorizer for one document. Each position in this vector here corresponds to a word and the number represents the number of times that word occurs in this document. If you want to look up the word associated with a particular numeric id, you can invoke the get_feature_names function and look up the word by index. The word at index 336 is until. Using the count vectorizer to numerically encode your documents results in very large feature vectors. In order to reduce the size of your feature vector representation, you might want to remove stop_words from your vocabulary. I have a file in my datasets folder containing all of the English sop words, which I'm going to read into a stop words data frame. Let's quickly sample this data frame to see some of the stop words in English. Here are words that are considered stop words, words that contain no information. I'm going to convert this data frame of stop words into a set, so we have just unique stop words here, and I'm going to pass these stop words into the CountVectorizer. So we get feature vectors that are smaller in size. Our vocabulary will not include stop words anymore. If you take a look at the length of the vocabulary for our six documents, now you can see that it's 294, down from the 367 that we had earlier because we've removed the stop words. Another technique that you can use to generate feature vectors from your text data is term frequency inverse document frequency, and for this we'll use the TfidfVectorizer in scikit-learn. Instantiate the TfidfVectorizer and pass in the stop words so that these are trimmed from your input documents. Call fit on your documents and this will generate TFIDF scores for all of the words in your vocabulary. The vocabulary property on the TFIDF vectorizer will give you the vocabular for your feature vectors. Observe that each word is associated with a numeric identifier, just like with the count vectorizer. The length of each feature vector to represent a document when you use TFIDF is the same as the size of your vocabulary. However, each word is represented using its TFIDF score. the TF score is higher is a word occurs more frequently in that document, and the IDF score will be higher when a word occurs less frequently across your entire corpus. The word is considered most significant then. You can observe the IDF scores for your individual words by accessing the IDF property on your TFIDF vectorizer. I've set up a pandas data frame here showing the IDF scores for each word in our vocabulary. Here are the words at the head of this data frame, the ones which have the lowest IDF scores. These are the most common words. If you take a look at the tail of the data frame, you'll see the words that occur rarely in the corpus. These have high IDF scores. You can generate TFIDF feature vectors of your input documents by calling fit_transform on your text data and these vectors will be in the form of a sparse matrix. We have six documents in our corpus and each document is represented using a feature vector of size 294, where 294 is equal to the size of our vocabulary. Here is a dense vector representation of the first document in our corpus. You can see that the individual words in this document are represented using the TFIDF scores. The TFIDF scores for a word are document specific. The same word can have a different score in a different document. Let's look at an example here. Let's get what word is represented using the numeric id 28. This is the word book. Now let's look at the TFIDF scores for this particular word across all of our documents. The word book is present in three out of six documents that the scores are non-zero and you can see that the TFIDF scores are different for the same word.

Loading and Transforming Images
In this demo, we'll perform a few image transformation and pre-processing techniques, and we'll also see how we can extract features from image data. In order to work with images, we'll install a number of additional Python libraries. The opencv library in Python is a Python wrapper around the original opencv library in C++. We'll also install the opencv-contrib library. The specific version that I've used here in this demo is here on screen, 3.4 .2 .16. We'll also install another commonly used library for image processing, that is scikit-images, which contains a number of different image processing algorithms. We'll use the image library in order to read in and display images and we'll also use the scipy library that's a scientific Python library for certain image transformations. I'm using all these libraries here to give you a brief overview of number of different image processing and transformation techniques. Go ahead and set up the import statements as you see here on screen. I'm going to load in a sample image here that I'll use to perform a few transformations. This is an image of a bird, which I got from the unsplashed site, which makes images freely available for any use, including for commercial purposes. Let's take a look at this image here, and you can see that it's a stork flying towards the left. The image library allows you to access the format, mode, and the size of this image. You can see that it's in the jpeg format. The mode is RGB indicating that it's a color image, and the size of the image is 500 pixels by 320 pixels. When you're feeding images into machine learning models, a common transformation that you might want to apply is to convert your images to grayscale. This makes your image data easier to work with. Here is a grayscale conversion using the image library. Pixels in a grayscale image can be represented using just one value, that for intensity. Let's take a look at the size of this image, it remains unchanged. Larger images require more processing and they're generally harder to work with. So another common transformation that you might perform is to generate a smaller size of the original image. This image object here allows us to generate a thumbnail representation. I've specified a 200 x 200 thumbnail, but observed that the original aspect ratio of the image has been maintained. The aspect ratio is the ratio of the width of an image to its height, and it's important that this be maintained in order to not distort an image. Observe here that the size of the original image was 500 pixels by 320 pixels, and the resized thumbnail is 200 pixels by 128 pixels. The aspect ratio has been maintained. Another function that you can use to resize an image is the resize function. Here I've resized the image to be 300 pixels by 300 pixels, and you can see that the resulting image has been clipped or cut off on the right to get a 300 by 300 image. The resized image has lost the original aspect ratio because of the truncation. If you're using images to train a machine learning model, it's a common use case to flip or distort the original image. Here I've transposed the original stork to be horizontally and vertically flipped. Let's take a look at the horizontally flipped image. The stork is now flying off to the right. And here is our vertically flipped image. The stork is now flying upside down. The image library in Python allows us to work with images in a simple and intuitive manner, which is why we looked at that first. But when you're using image data for machine learning models, it's common to work with images in the form of an array. Every image, whether it's a color image or a grayscale image, can be represented using a three-dimensional matrix. Observe that to represent a pixel in our color image, we need three values, R, G, and B, and that is the size of our third dimension here in this matrix. You can visualize this array representation of your image using matplotlib as well. Here is our stork flying left, the original stork. Now that you have this image in matrix or array form, if you want to convert it to grayscale, you can use the rgb2gray function available in scikit-image. And here is the grayscale representation of our original stork. In a grayscale image, you require just one value to represent a pixel, that is the intensity, which means that your image can be represented as a 2D matrix. If you add a third dimension for the pixel representation, it's size would be 1. The scikit-image Python libraries, as well as numpy, can be used to perform a number of different transformations on your images represented as arrays. Here is the resize function from scikit-image, which allows you to resize your image. Setting anti_aliasing to true will reduce the distortions that might be introduced in your resized lower resolution image. Our resized stork is now 224 pixels by 224 pixels. Let's display this using matplotlib, and here is our resized stork flying left.

Extracting Features from Images
From image transformations, let's move on to extracting features from our image data. This is the multichannel rgb stork image that we've been working with. This is an image represented as an array. I'm now going to extract features in the form of patches from this image. Patch extraction can be done using the extract_patches_2d function available in scikit-learn, and I want each patch to be 64 pixels by 64 pixels. If you take a look at the shape of the resulting patches, you'll see that it's a four-dimensional vector. Each patch is 64 pixels by 64 pixels. Because this is a color image, we have three values to represent a pixel, that is the size of the last dimension, and this first dimension here represents the number of patches that we extracted. Patch extraction can be thought of as sliding a 64 by 64 window across your entire image horizontally, as well as vertically, giving us more than 100, 000 patches. Patch extraction is often used to learn details from the underlying image, such as in dictionary learning algorithms. Each patch here is an image by itself. Let's display one of these patches, and you can see it represents a portion of our stork. Patch extraction from image data can be used to feed into dictionary learning algorithms, which learn sparse representations of dense image data. Dictionary learning can then be used to denoise noisy images. Image patches are thus features of an image that can serve as an input to other image processing algorithms. Let's now extract a different kind of feature from our underlying image. I'm going to import the data, io, and filters object from scikit-image, and this time we'll use the coins image that is available built in in scikit-image. The coins image is represented as a multi-dimensional array, and you can see the size of this matrix, it's a 303 pixels by 384 pixels image. Before we apply feature detection techniques to this image, let's see what this original coins image looks like. An image of about 25 or 26 different coins. There are different features that you might interesting based on what you want to do with an image. A common feature detection technique is edge detection, and the sobel edge detector is one algorithm which can perform edge detection for us. Edge detectors are used to find the boundaries of objects in images and edge detection algorithms look for discontinuities in images, such as discontinuities in brightness, to find where the edges are located. The result of a sobel edge detection algorithm in scikit-image is an image with the edges detected. Let's take a look at this edge detected image, and you can see that the boundaries of the coins in the original image have been detected and highlighted. Image processing offers a number of algorithms to detect and extract different kinds of features from your image data, for example, you can use the peak_local_max function in scikit-image to find the local maximas that exist in your image. Let's invoke the peak_local_max function on our coins image to extract local maximas. We'll then plot the original image and also plot the coordinates of the local maxima, and let's see what the result looks like. Here is the original image and all the local maximas. That is, points having a high intensity have been highlighted here in this image.

Detecting Keypoints and Descriptors to Perform Image Matching
Interesting features in images are often represented using key points and descriptors. A key point is an interest point in an image, and a descriptor is used to describe the characteristics of a key point. Key points and descriptors are often used together to represent a feature in an image. Here I'm going to use open cv2 to read in an image of the leaning tower of Pisa, which I stored in the pisa1 variable. As you can see, it's a multichannel color image, 426 by 640 pixels. Let's take a look at this image using matplotlib, and here is the leaning tower of Pisa. All the images used in this demo I've downloaded from pexels.com, which makes images freely available for everyone to use. I'm now going to use opencv to convert this image to a grayscale image using the cvtColor function. Once I have the grayscale representation for this image, I'm going to use SIFT, or the scale invariant feature transform algorithm to detect and extract features in the underlying image. The opencv library contains an object which implements the SIFT algorithm. You can invoke the detect function on your SIFT algorithm, which will detect key points that exist in your image. Key points are spatial locations or points in an image that defines what is interesting or what stands out in the underlying image. The key points are available here in the form of an array of KeyPoint objects. Opencv allows us to visualize these key points overlaid over the original image. Use cv2.drawKeyPoints to overlay the key points over the leaning tower of Pisa, and I'm now going to write this out to a jpeg file. Let's read in this jpeg file using scikit-image and let's display this, and we'll see what the leaning tower of Pisa looks like with the key points highlighted. And that's exactly what you see here on screen. Observe that the key points are often present along edges or discontinuities in your image. Those are the interesting regions. Key points contain additional information about the points of interest in an image and you can use rich key points in order to visualize this additional information. Simply draw key points using this flag here, DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS. Every key point will now be represented using a circle where the circle represents the size and orientation of the key point. I'll now write out a file with the original image with the rich key points visualized as an overlay. And we'll now read this file in from our local disk and display it using matplotlib. Observe that how the key points are displayed are different in this image. Every key point is represented using a circle and a little line representing the radius of a circle. Under the hood, the way key points are discovered is by decomposing the image at different scales. The larger the circle that you see here on screen, larger the scale at which the key point was detected. Observe that the key points have this little line here within the circle and this line represents the orientation of this key point. This is an important feature, that is the orientation of the key point that is used in feature matching algorithms. The size and orientation of key points are what we'd use to match key points across images. Every key point is associated with a descriptor, a descriptor is just a numerical representation of additional information about a key point. Here are the descriptors that were extracted from our first pisa1 image. I'm now going to read in another image of the leaning tower of Pisa using cv2.imread. The shape of this image is the same as the original image that we read in. It's also a multichannel color image. I'm now going to use a matplotlib to see what this original image looks like. It's the leaning tower of Pisa once again, but it's a different image. Once again, I'll use opencv to convert this image to the grayscale format, and I'm going to call sift.detectAndCompute on this grayscale image to detect and compute key points and descriptors. Once we have the key points and descriptors from both images, we can use the BFMatcher, that is the brute force matcher, to match the key points across images. Instantiate the brute force matcher and invoke bf.match and pass in the descriptors from both of our leaning tower of Pisa images. This will perform a brute force comparison of each descriptor in each of the two images to find which descriptors match. Once the matches have been detected, let's visualize the top 20 matches. I'm going to invoke cv2.drawMatches and pass in both of the original images, along with the key points and corresponding matches. The result is an image which matches key point across both of our leaning tower of Pisa images, and this is what the brute force matcher has found, key points that seem similar across the two images.

Extracting Text from Images Using OCR
In this demo, we'll perform optical character recognition, or OCR, to extract text from images. And for this we'll use the open source tesseract engine that was made originally available by Google. In order to install tesseract on a MacOS or a Linux machine, you'll first need to install Brew, which you can do using this command here. Brew is a very popular package manager for Linux and the MacOS and this command here is available at brew.sh. Once Brew has been installed on your local machine, you can go ahead and install tesseract. Simply invoke the brew install tesseract function on your terminal window, and this will install the tesseract engine onto your local machine. And once tesseract has been installed, you can use tesseract for optical character recognition using the pytesseract library. Pytesseract is a Python wrapper over the tesseract engine, and you can get it using a simple pip install. Go ahead, install pytesseract, also install the pillow library for image processing using a pip install. Set up the import statements for the libraries and modules that we'll need for this demo. Let's go ahead and read in an image which contains some text. This is a motivational quote. Let's take a look at the format, mode, and size of this quote image. You can see that it's in the jpeg format, it's an RGB image, and it's a fairly large image, 1920 pixels by 1920 pixels. I'm curious about what exactly this motivational quote says, let's display this image to screen. You can see that it's indeed motivational. Let's see if you can extract the text from this image using OCR. Text detection and extraction using pytesseract is very straightforward. Simply invoke the image_to_string function, pass in your image, and specify the language. Let's print out the text that we extracted and you can see that tesseract did a great job. The only character which it got wrong is the u in environment. It got a u instead of a v. That was a fairly simple image with some simple text. Let's extract text from an image that contains much more text and is very dense. This is an image in the png format, it's an RGB image, and it contains the text which is descriptive of play called Dan Druce. This is a scene from Dan Druce, and I got this image from Wikipedia. This is an image that can be freely used under the creative comments license. Once again, let's extract the English text from this image using pytesseract.image_to_string, and here is the text that we got. Once again, tesseract did a pretty good job. But what if the original image wasn't very clear to begin with? I'm going to add a little Gaussian blur to the original image. I'm specify a sigma value here of 1, which adds just a little bit of distortion, and here is our distorted image. Let's see if pytesseract can now perform OCR and extract text. Invoke pytessert.image_to_string, and you can see that pytesseract works, even if the original image is a little bit distorted or blurry. I'm now going to apply a different Gaussian filter to this image, which increases the amount of distortion present. This is an image with additional blur introduced. And you'll see that pytesseract, at this point, fails. It can't really extract the text well. It extracted just a few words. What if the original image didn't have the text the right way up? I'm going to flip the image from top to bottom. And let's take a look at the flipped image here. Here is our upside-down motivational quote. And if you use tesseract to extract text from this image, you'll find that tesseract fails once again. It does a little text extraction. It can't really identify that the original image was upside down.

Extracting Features from Dates
In this demo, we'll see how we can extract features from dates. We'll start off on a brand new notebook here. If you want to visualize and plot date related information using matplotlib, you'll need to register a matplotlib date converter. Invoke the register_matplotlib_converters in order to do this. In order to have some real-world data with date information embedded, I'm going to work with apple stock prices over the last 9 or 10 years. You can see that our data frame contains stock price information for a particular trading day. This data is freely available at finance.yahoo .com, and that's where I downloaded it from. I'm going to shuffle up this data by invoking the sample function on my data frame so that the records are no longer in chronological order. I'm now going to reset the indices of the original pandas data frame and let's take a look at the sample of the data available now. You can see that the Date column contains dates in any random order. Let's invoke the info function to take a look at the data types for the different column values. You can see that the Date column is of type object. So it's not really represented as a datetime object, it's just object. You can convert date representations to the numpy datetime format by invoking pd.to_datetime. The Date column now has the data type, datetime64. This is the datetime object natively supported by numpy. Let's take a look at the shape of this data frame. You can see that there are roughly 2200 records representing trading days over about 10 years. Now that date information is represented in a datetime format, you can sort your data frame based on the Date column. If you take a look at the head of the data frame, you'll see that our trading data starts off on the first of September 2010. Once the data frame is sorted, the index of the original data frame is all jumbled up. I'm going to reset the index so that it starts at zero once again. Now let's say you want the dates in your data frame to be represented using the Python datetime representation. You can apply this transformation using a lambda function. This lambda function will apply on the numpy datetime values and call to_pydatetime, which will convert the dates to the Python representation. The Python datetime representation is also called datetime64, so that makes it a little confusing. But it supports additional operations that allow you to extract specific details from your dates. For example, you can use the d.isoweekday to see whether a particular date represents a weekday or not, and if it's a weekday, you can identify the specific day using the numeric id that is extracted. You can see that first September 2010 was a Wednesday represented by the id 3. Third September 2010 was a Friday represented by the id 5, that is the 5th weekday. Once you have your dates represented using the Python datetime format, you can use these dates to extract specific bits of information, such as the year. The Python datetime.strptime method passes a string representing a time, and this string can then be used to extract information. And the format of the string is what we specify here. We can then extract the year property associated with the date. Let's extract the year from all of our dates into the Year column. We can perform a similar operation to extract the months associated with our dates as well. The datetime.strptime function to convert the string date to a specified format and then we'll extract the month. You can see that the months are represented using numeric ids from 1 to 12. Once you've extracted the specific features that you want from your Date column, you can visualize your data in multiple ways. Here we'll plot a line plot of the date versus the closing price of Apple stock, and you can see how the Apple stock has gone up over the last 10 years. Let's see if there is a month wise pattern to this price. I'm going to group my original data frame by month and calculate the average price on a per month basis. This is an average price for Apple stock, calculated for each month across all of the previous 10 years. Now that we've extracted month specific information for each of our dates, let's plot the closing price of Apple, that is the average closing price on a per month basis. And this is what the resulting visualization looks like.

Working with Geospatial Features
In this demo, we'll see how we can represent and work with features extracted from geographical data. We start this demo off in a brand new notebook, and in order to work with geospatial data, we'll need to install an additional library called geopandas. Now I've found geopandas an incredibly interesting and useful library. This is an open source project associated with pandas, which makes it easier to work with geospatial data in Python. Geographical data is represented using geometric types and you can perform spatial operations on this geographical data. Once you have geopandas installed on your machine, there is another Python library that we'll install called Descartes. This library uses shapely or GeoJSON-like geometric objects to represent matplotlib paths or patches, allowing us to visualize geographical data using matplotlib visualization. Shapely and GeoJSON are both different formats that you can use to represent geographical data. With these libraries installed, you are ready to set up your import statements, import geopandas, and set your pandas display options to display just 10 rows of your data frame. I'm going to instantiate a data frame with geographical data. I'm going to use this data frame to represent a few well known cities in different countries across South America. And here are the latitudes and longitudes of these individual cities. This is a regular pandas data frame with city, country, lat, and long information. Now let's convert this to a GeoDataFrame available in geopandas. A GeoDataFrame is what you'll use to represent geospatial information, and you can then use it to perform interesting operations. The latitude and longitude information from our original data frame needs to be converted to shapely objects representing a point. These are points that will represent positions on a map. Let's take a look at this GeoDataFrame that we just set up. We have all of the original information and an additional geometry column, which has point objects within it. Now it's possible to visualize these cities on a map of the world, and for this, you'll need to read in a file, which contains a mapping of the world. This is a specific dataset available built in in the geopandas library. This is a naturalearth dataset containing a world map representation using GeoDataFrames. And the interesting thing is, you can actually filter this map just like you would filter any other pandas data frame. I want to visualize South America using the color white with the edge color black. Plot this mapping using matplotlib, and here is what our map of South America looks like. Observe that the cities that we have mentioned earlier have been plotted in red. Gdf.plot simply picked up the right information from our geopandas data frame in order to plot this representation. It's possible for you to use external datasets with geopandas as well. Here is a dataset with geometric polygons representing 247 countries in the world, and the original URL for this dataset is what you see here on screen. You can see that in addition to geographic information, this dataset contains a number of additional information about individual countries. The country name is present in this column highlighted here. Let's see the type of this countries data frame, and you can see that it's a geodataframe. Once you know this, you can always invoke the plot function on your data frame to visualize a map of all of the countries in the world, and here is what the map looks like. Observe that the polygonal shape for each country is clearly delineated here. This polygon representation of each country is available in the geometric column of this countries geodataframe. Let's take a look at this column, and you can see that the individual values are polygons. These are polygons, which is the geometric shape used to represent a filled area on a map, such as a country, continent, or some region. In addition to geospatial data, this countries data frame contains other information about countries as well. Let's extract a few of these columns and see what they contain. You can see that we have the income group for a country, the estimated population for a country, etc. When you have geodataframes, you can perform geographic filtering. For example, I want to view all of the countries in the continent Asia. This filtered data is stored in the Asia data frame, you can call asia.plot, and here is a map of just all of the countries in Asia. Just like we have datasets, with geographical information about countries of the world, we have datasets, giving us city specific information as well, that can be read into a geodataframe. All of this data is available at the same URL that I showed you earlier in this demo. Let's take a look at a few cities here, and you can see that these cities are represented using a point object, present in the geometry column. You can access the point information for individual cities using the geometry column and specific indices. Here is the city at index 0 and at index 241. I'll now extract the rows corresponding to these two cities so that we can view additional information, and you can see that the two cities are Vatican City and Singapore. Now we have the two cities, which is also a geodataframe, if you just plot this geodataframe, you'll only get the points representing the two cities, but no corresponding world map. In order to plot these cities on the map, you need to set up the axes to be from the countries data frame and then plot these two cities. The countries data frame is what contains our world map. And here are the cities plotted on the world map. At the same URL that we saw earlier, you have geographic datasets representing the rivers of the world as well, and that's what I'm going to read in and display here in this rivers geodataframe. A river is not a point or a polygon and it's represented by a third geometric shape called a line string. A line string is essentially a sequence of points. I'll now display our world map invoking countries.plot. I want the countries to be displayed using an edgecolor and no facecolor. And I'll now overlay this plot with the river information. And I'm going to set the x limits and y limits of my geographical data so that I zoom into Africa. And here is our world map representation of the continent of Africa and a little bit of Asia, along with their rivers.

Summary and Further Study
And with this, we come to the very end of this action-packed, demo-based model where we saw how we could extract features from different kinds of data. We started off by working with text data. We saw how we could tokenize text, perform lemmatization and we saw what it meant to normalize text data. We then converted text to feature vectors. We then moved on to working with image data. We represented images as matrices and we then applied a number of preprocessing techniques on image data. We then performed feature extraction using the SIFT algorithm, and we used these descriptors to perform feature matching. We then moved on to extracting text from images using OCR, or optical character recognition, using the pytesseract API for the tesseract engine. We then saw how it is to work with time series data, such as datetime, and we saw how we could extract features, such as day of the week, month of the year, and year information. And we finally rounded off by working with geospatial data in geopandas. And with this, we come to the very end of this course on preparing data for feature engineering and machine learning. If feature engineering is what you're interested in, there are a bunch of other courses on Pluralsight that you could watch. Building Features from Nominal Data will show you how you can work with categorical data. Building Features from Text Data focuses on natural language processing and feature extraction from text. And finally, Building Features from Image Data will show you how you can work with images. And it's time for me to say goodbye. That's it from me here today. Thank you for listening.